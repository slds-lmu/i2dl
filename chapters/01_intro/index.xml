<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Topic 1: Introduction on Introduction to Machine Learning (I2ML)</title><link>https://slds-lmu.github.io/i2dl/chapters/01_intro/</link><description>Recent content in Topic 1: Introduction on Introduction to Machine Learning (I2ML)</description><generator>Hugo</generator><language>en-us</language><atom:link href="https://slds-lmu.github.io/i2dl/chapters/01_intro/index.xml" rel="self" type="application/rss+xml"/><item><title>Chapter 01.01: Introduction</title><link>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-01-intro/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-01-intro/</guid><description>&lt;p>In this section, we introduce the relationship of DL and ML, give a basic intro about feature learning, and discuss the use-cases and data types for DL methods.&lt;/p></description></item><item><title>Chapter 01.02: Single Neuron</title><link>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-02-single-neuron/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-02-single-neuron/</guid><description>&lt;p>In this section we explain the graphical representation of a single neuron and describe affine transformations and non-linear activation functions. Moreover, we talk about the hypothesis space of a single neuron and name some typical loss functions.&lt;/p></description></item><item><title>Chapter 01.03: XOR Problem</title><link>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-03-xor/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-03-xor/</guid><description>&lt;p>In this subsection, we present the XOR problem, a famous example a single neuron can not solve but a single hidden layer net can solve.&lt;/p></description></item><item><title>Chapter 01.04: Single Hidden Layer NN</title><link>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-04-singlenn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-04-singlenn/</guid><description>&lt;p>In this subchapter, we introduce the architecture of single hidden layer neural networks and discuss the advantages of hidden layers. In addition, we present some typical (non-linear) activation functions.&lt;/p></description></item><item><title>Chapter 01.05: Single Hidden Layer Networks for Multi-Class Classification</title><link>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-05-nn4multiclass/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-05-nn4multiclass/</guid><description>&lt;p>In this subsection, we discuss neural network architectures for multi-class classification, the softmax activation function as well as the softmax loss.&lt;/p></description></item><item><title>Chapter 01.06: MLP: Multi-Layer Feedforward Neural Networks</title><link>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-06-multilayernn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-06-multilayernn/</guid><description>&lt;p>In this subchapter, we present the architecture of deep neural networks and introduce deep neural networks as chained functions.&lt;/p></description></item><item><title>Chapter 01.07: Matrix Notation</title><link>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-07-matrixnotation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-07-matrixnotation/</guid><description>&lt;p>In this section we explain the compact representation of neural networks, the vector notation for neuron layers and both the vector and matrix notation for bias and weight parameters.&lt;/p></description></item><item><title>Chapter 01.08: Universal Approximation</title><link>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-08-universalapproximation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-08-universalapproximation/</guid><description>&lt;p>Here we present the universal approximation theorem for one-hidden-layer neural networks. In addition, we discuss the pros and cons of a low approximation error.&lt;/p></description></item><item><title>Chapter 01.09: Brief History</title><link>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-09-history/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-09-history/</guid><description>&lt;p>In this subsection we present an overview of the history of DL development.&lt;/p></description></item></channel></rss>
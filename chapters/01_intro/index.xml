<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Topic 1: Introduction on Introduction to Machine Learning (I2ML)</title><link>https://slds-lmu.github.io/i2dl/chapters/01_intro/</link><description>Recent content in Topic 1: Introduction on Introduction to Machine Learning (I2ML)</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://slds-lmu.github.io/i2dl/chapters/01_intro/index.xml" rel="self" type="application/rss+xml"/><item><title>Chapter 01.01: Introduction</title><link>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-01-intro/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-01-intro/</guid><description>&lt;p>In this section, we introduces the relationship of DL and ML, give basic intro about feature learning, and discuss the use-cases and data types for DL methods.&lt;/p></description></item><item><title>Chapter 01.02: Single Neuron</title><link>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-02-single-neuron/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-02-single-neuron/</guid><description>&lt;p>In this section we explain the graphical representation of a single neuron and describe affine transformations and non-linear activation functions. Moreover, we talk about the hypothesis spaces of a single neuron and name some typical loss functions.&lt;/p></description></item><item><title>Chapter 01.03: XOR Problem</title><link>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-03-xor/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-03-xor/</guid><description>&lt;p>Example problem a single neuron can not solve but a single hidden layer net can!&lt;/p></description></item><item><title>Chapter 01.04: Single Hidden Layer NN</title><link>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-04-singlenn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-04-singlenn/</guid><description>&lt;p>We introduce architecture of single hidden layer neural networks and discuss the advantage of hidden layers. Then, we explain the typical (non-linear) activation
functions.&lt;/p></description></item><item><title>Chapter 01.05: Single Hidden Layer Networks for Multi-Class Classification</title><link>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-05-nn4multiclass/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-05-nn4multiclass/</guid><description>&lt;p>In this section, we discuss a neural network architectures for multi-class classification, softmax activation function as well as the Softmax loss.&lt;/p></description></item><item><title>Chapter 01.06: MLP: Multi-Layer Feedforward Neural Networks</title><link>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-06-multilayernn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-06-multilayernn/</guid><description>&lt;p>Architectures of deep neural networks and deep neural networks as chained functions are the learning goal of this part.&lt;/p></description></item><item><title>Chapter 01.07: Optimization</title><link>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-07-matrixnotation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-07-matrixnotation/</guid><description>&lt;p>In this section we learn about compact representation of neural network, vector notation for neuron layers, vector and matrix notation of bias and weight parameters.&lt;/p></description></item><item><title>Chapter 01.08: Universal Approximation</title><link>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-08-universalapproximation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-08-universalapproximation/</guid><description>&lt;p>Universal approximation theorem for one-hidden-layer neural networks and pros and cons of a low approximation error are the learning goal of this section.&lt;/p></description></item><item><title>Chapter 01.09: Brief History</title><link>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-09-history/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-09-history/</guid><description>&lt;p>We overview history of DL development.&lt;/p></description></item></channel></rss>
<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Topic 2: Optimization Part-1 on Introduction to Machine Learning (I2ML)</title><link>https://slds-lmu.github.io/i2dl/chapters/02_optimization1/</link><description>Recent content in Topic 2: Optimization Part-1 on Introduction to Machine Learning (I2ML)</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://slds-lmu.github.io/i2dl/chapters/02_optimization1/index.xml" rel="self" type="application/rss+xml"/><item><title>Chapter 02.01: Loss Functions for Regression</title><link>https://slds-lmu.github.io/i2dl/chapters/02_optimization1/02-01-basic-training/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/02_optimization1/02-01-basic-training/</guid><description>&lt;p>Empirical risk minimization, gradient descent, and stochastic gradient descent are three subtopic which we explain in this chapter.&lt;/p></description></item><item><title>Chapter 02.02: Chain Rule and Computational Graphs</title><link>https://slds-lmu.github.io/i2dl/chapters/02_optimization1/02-02-computational-graph/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/02_optimization1/02-02-computational-graph/</guid><description>&lt;p>In this section, we explain the chain rule of calculus, and it&amp;rsquo;s computational graphs.&lt;/p></description></item><item><title>Chapter 02.03: Basic Backpropagation 1</title><link>https://slds-lmu.github.io/i2dl/chapters/02_optimization1/02-03-backprob1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/02_optimization1/02-03-backprob1/</guid><description>&lt;p>This section introduces forward and backward passes, chain rule, and the details of backprop in deep learning.&lt;/p></description></item><item><title>Chapter 02.04: Basic Backpropagation 2</title><link>https://slds-lmu.github.io/i2dl/chapters/02_optimization1/02-04-backprob2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/02_optimization1/02-04-backprob2/</guid><description>&lt;p>We continue our discussion about backpropagation in formalism and recursion.&lt;/p></description></item><item><title>Chapter 02.05: Hardware and Software</title><link>https://slds-lmu.github.io/i2dl/chapters/02_optimization1/02-05-hardware/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/02_optimization1/02-05-hardware/</guid><description>&lt;p>This section introduces GPU training for accelerated learning of neural networks, software for hardware support, and deep learning software platforms.&lt;/p></description></item></channel></rss>
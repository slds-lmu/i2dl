<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Topic 2: Optimization - Part I on Introduction to Machine Learning (I2ML)</title><link>https://slds-lmu.github.io/i2dl/chapters/02_optimization1/</link><description>Recent content in Topic 2: Optimization - Part I on Introduction to Machine Learning (I2ML)</description><generator>Hugo</generator><language>en-us</language><atom:link href="https://slds-lmu.github.io/i2dl/chapters/02_optimization1/index.xml" rel="self" type="application/rss+xml"/><item><title>Chapter 02.01: Basic Training</title><link>https://slds-lmu.github.io/i2dl/chapters/02_optimization1/02-01-basic-training/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/02_optimization1/02-01-basic-training/</guid><description>&lt;p>This subchapter covers essential principles of neural network training, starting with empirical risk minimization (ERM) and gradient descent (GD). Additionally, it introduces stochastic gradient descent (SGD) as a computationally efficient alternative to GD.&lt;/p></description></item><item><title>Chapter 02.02: Chain Rule and Computational Graphs</title><link>https://slds-lmu.github.io/i2dl/chapters/02_optimization1/02-02-computational-graph/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/02_optimization1/02-02-computational-graph/</guid><description>&lt;p>In this subsection, we explain the chain rule of calculus, and the corresponding computational graphs.&lt;/p></description></item><item><title>Chapter 02.03: Basic Backpropagation I</title><link>https://slds-lmu.github.io/i2dl/chapters/02_optimization1/02-03-backprob1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/02_optimization1/02-03-backprob1/</guid><description>&lt;p>This subsection introduces forward and backward passes, the chain rule, and the details of backpropagation in deep learning.&lt;/p></description></item><item><title>Chapter 02.04: Basic Backpropagation II</title><link>https://slds-lmu.github.io/i2dl/chapters/02_optimization1/02-04-backprob2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/02_optimization1/02-04-backprob2/</guid><description>&lt;p>In this subsection we focus on the formalism of backpropagation and the concept of recursion in this context.&lt;/p></description></item><item><title>Chapter 02.05: Hardware and Software</title><link>https://slds-lmu.github.io/i2dl/chapters/02_optimization1/02-05-hardware/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/02_optimization1/02-05-hardware/</guid><description>&lt;p>This subsection introduces GPU training for accelerated learning of neural networks, software for hardware support, and deep learning software platforms.&lt;/p></description></item></channel></rss>
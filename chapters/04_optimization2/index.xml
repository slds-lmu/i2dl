<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Topic 04: Optimization - Part II on Introduction to Machine Learning (I2ML)</title><link>https://slds-lmu.github.io/i2dl/chapters/04_optimization2/</link><description>Recent content in Topic 04: Optimization - Part II on Introduction to Machine Learning (I2ML)</description><generator>Hugo</generator><language>en-us</language><atom:link href="https://slds-lmu.github.io/i2dl/chapters/04_optimization2/index.xml" rel="self" type="application/rss+xml"/><item><title>Chapter 04.01: Challenges in Optimization</title><link>https://slds-lmu.github.io/i2dl/chapters/04_optimization2/04-01-challenges_opt/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/04_optimization2/04-01-challenges_opt/</guid><description>&lt;p>In this subsection, we summarize several of the most prominent challenges regarding training of deep neural networks such as ill-conditioning, local minima, saddle points, cliffs and exploding gradients.&lt;/p></description></item><item><title>Chapter 04.02: Measures Regression</title><link>https://slds-lmu.github.io/i2dl/chapters/04_optimization2/04-02-adv-opt/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/04_optimization2/04-02-adv-opt/</guid><description>&lt;p>In this section we introduce several advanced techniques for optimization of neural network such as learning rate schedules, adaptive learning rates, and batch normalization.&lt;/p></description></item><item><title>Chapter 04.03: Modern Activation Functions</title><link>https://slds-lmu.github.io/i2dl/chapters/04_optimization2/04-03-activation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/04_optimization2/04-03-activation/</guid><description>&lt;p>In this subchapter, we explain challenges in optimization related to activation functions. In addition, we introduce activations for both hidden and output units.&lt;/p></description></item><item><title>Chapter 04.04: Network Initialization</title><link>https://slds-lmu.github.io/i2dl/chapters/04_optimization2/04-04-init/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/04_optimization2/04-04-init/</guid><description>&lt;p>In this part we explain why initialization is crucial for neural network training. We introduce the concept of random weight initialization and explain initialization approaches for the biases of a NN.&lt;/p></description></item></channel></rss>
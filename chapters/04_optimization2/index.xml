<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Chapter 04: Performance Evaluation on Introduction to Machine Learning (I2ML)</title><link>https://slds-lmu.github.io/i2dl/chapters/04_optimization2/</link><description>Recent content in Chapter 04: Performance Evaluation on Introduction to Machine Learning (I2ML)</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://slds-lmu.github.io/i2dl/chapters/04_optimization2/index.xml" rel="self" type="application/rss+xml"/><item><title>Chapter 04.01: Generalization Error</title><link>https://slds-lmu.github.io/i2dl/chapters/04_optimization2/04-01-generalization-error/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/04_optimization2/04-01-generalization-error/</guid><description>&lt;p>It is a crucial part of machine learning to evaluate the performance of a learner. We will explain the concept of generalization error and the difference between inner and outer loss.&lt;/p></description></item><item><title>Chapter 04.02: Measures Regression</title><link>https://slds-lmu.github.io/i2dl/chapters/04_optimization2/04-02-measures-regression/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/04_optimization2/04-02-measures-regression/</guid><description>&lt;p>In this section we familiarize ourselves with essential performance measures for regression. In particular, mean squared error (MSE), mean absolute error (MAE), and a straightforward generalization of $R^2$ are discussed.&lt;/p></description></item><item><title>Chapter 04.03: Training Error</title><link>https://slds-lmu.github.io/i2dl/chapters/04_optimization2/04-03-train/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/04_optimization2/04-03-train/</guid><description>&lt;p>There are two types of errors: training errors and test errors. The focus of this section is on the training error and related difficulties.&lt;/p></description></item><item><title>Chapter 04.04: Test Error</title><link>https://slds-lmu.github.io/i2dl/chapters/04_optimization2/04-04-test/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/04_optimization2/04-04-test/</guid><description>&lt;p>While we can infer some information about the learning process from training errors (e.g., the state of iterative optimization), we are truly interested in generalization ability, and thus in the test error on previously unseen data.&lt;/p></description></item><item><title>Chapter 04.05: Overfitting &amp; Underfitting</title><link>https://slds-lmu.github.io/i2dl/chapters/04_optimization2/04-05-overfitting-underfitting/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/04_optimization2/04-05-overfitting-underfitting/</guid><description>&lt;p>In machine learning, we are interested in a model that captures the true underlying function and still generalizes well to new data.
When the model fails on the first task, we speak of underfitting, and both train and test error will be high.
On the other hand, learning the training data very well at the expense of generalization ability is referred to as overfitting and usually occurs when there is not enough data to tell our hypotheses apart.
We will show you examples of this behavior and how to diagnose overfitting.&lt;/p></description></item><item><title>Chapter 04.06: Resampling 1</title><link>https://slds-lmu.github.io/i2dl/chapters/04_optimization2/04-06-resampling-1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/04_optimization2/04-06-resampling-1/</guid><description>&lt;p>Different resampling techniques help to assess the performance of a learner while avoiding potential quirks resulting from a single train-test split. We will introduce cross-validation (with and without stratification), bootstrap and subsampling.&lt;/p></description></item><item><title>Chapter 04.07: Resampling 2</title><link>https://slds-lmu.github.io/i2dl/chapters/04_optimization2/04-07-resampling-2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/04_optimization2/04-07-resampling-2/</guid><description>&lt;p>We provide a deep dive on resampling, showing its superiority to holdout
splitting and analyzing the bias-variance decomposition of its MSE.
We further point out the dependence between CV fold results and that
hypothesis testing is therefore not applicable, and give some practical tips
to choose resampling strategies.&lt;/p></description></item><item><title>Chapter 04.08: Measures Classification</title><link>https://slds-lmu.github.io/i2dl/chapters/04_optimization2/04-08-measures-classification/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/04_optimization2/04-08-measures-classification/</guid><description>&lt;p>Analogous to regression, we consider essential performance measures for classification. As a classifier predicts either class labels or scores/probabilities, its performance can be evaluated based on these two notions. We show some performance measures for classification, including misclassification error rate (MCE), accuracy (ACC) and Brier score (BS). In addition, we will see confusion matrices and learn about costs.&lt;/p></description></item><item><title>Chapter 04.09: ROC Basics</title><link>https://slds-lmu.github.io/i2dl/chapters/04_optimization2/04-09-rocbasics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/04_optimization2/04-09-rocbasics/</guid><description>&lt;p>From the confusion matrix we can calculate a variety of ROC metrics. Among others, we will explain true positive rate, negative predictive value and the $F1$ measure.&lt;/p></description></item><item><title>Chapter 04.10: ROC Curves</title><link>https://slds-lmu.github.io/i2dl/chapters/04_optimization2/04-10-roccurves/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/04_optimization2/04-10-roccurves/</guid><description>&lt;p>In this section, we explain the ROC curve and how to calculate it. In addition, we will present the AUC as a global performance measure that integrates over all possible thresholds.&lt;/p></description></item><item><title>Chapter 04.11: Partial AUC &amp; Multi-Class AUC</title><link>https://slds-lmu.github.io/i2dl/chapters/04_optimization2/04-11-partialauc-mcauc/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/04_optimization2/04-11-partialauc-mcauc/</guid><description>&lt;p>We discuss both the partial AUC, which restricts the AUC to the relevant area
for a specific application, and possible extensions of the AUC to multi-class
classification.&lt;/p></description></item><item><title>Chapter 04.12: Precision-Recall Curves</title><link>https://slds-lmu.github.io/i2dl/chapters/04_optimization2/04-12-prcurves/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/04_optimization2/04-12-prcurves/</guid><description>&lt;p>Besides plotting TPR against FPR to obtain the ROC curve, it sometimes makes
sense to instead consider precision (= PPV) vs recall (= TPR), especially when
data are imbalanced.&lt;/p></description></item><item><title>Chapter 04.13: AUC &amp; Mann-Whitney-U Test</title><link>https://slds-lmu.github.io/i2dl/chapters/04_optimization2/04-13-auc-mwu/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/04_optimization2/04-13-auc-mwu/</guid><description>&lt;p>We demonstrate that the AUC is equivalent to the normalized test statistic in
the Mann-Whitney-U test, both of which are effectively rank-based metrics.&lt;/p></description></item></channel></rss>
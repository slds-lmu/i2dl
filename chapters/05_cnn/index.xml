<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Chapter 05: k-Nearest Neighbors (k-NN) on Introduction to Machine Learning (I2ML)</title><link>https://slds-lmu.github.io/i2dl/chapters/05_cnn/</link><description>Recent content in Chapter 05: k-Nearest Neighbors (k-NN) on Introduction to Machine Learning (I2ML)</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://slds-lmu.github.io/i2dl/chapters/05_cnn/index.xml" rel="self" type="application/rss+xml"/><item><title>Chapter 05.01: k-Nearest Neighbors (k-NN)</title><link>https://slds-lmu.github.io/i2dl/chapters/05_cnn/05-01-knn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/05_cnn/05-01-knn/</guid><description>&lt;p>We demonstrate that distances in feature space are crucial in \(k\)-NN regression / classification and show how we can form predictions by averaging / majority vote. In this, \(k\)-NN is a very local model and works without distributional assumptions.&lt;/p></description></item></channel></rss>
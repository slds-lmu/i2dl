<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Topic 06: Convolutional Neural Networks - Part II on Introduction to Machine Learning (I2ML)</title><link>https://slds-lmu.github.io/i2dl/chapters/06_moderncnn/</link><description>Recent content in Topic 06: Convolutional Neural Networks - Part II on Introduction to Machine Learning (I2ML)</description><generator>Hugo</generator><language>en-us</language><atom:link href="https://slds-lmu.github.io/i2dl/chapters/06_moderncnn/index.xml" rel="self" type="application/rss+xml"/><item><title>Chapter 06.01: 1D/ 2D/ 3D Convolutions</title><link>https://slds-lmu.github.io/i2dl/chapters/06_moderncnn/06-01-con1d/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/06_moderncnn/06-01-con1d/</guid><description>&lt;p>In this subchapter we present the various types of convolutions in CNNs — 1D, 2D, and 3D — each suited for specific data structures and applications. One-dimensional convolutions process sequential data like time series, two-dimensional convolutions handle spatial data like images, and three-dimensional convolutions analyze volumetric data like videos or MRI scans.&lt;/p></description></item><item><title>Chapter 06.02: Dilated Convolutions and Transposed Convolutions</title><link>https://slds-lmu.github.io/i2dl/chapters/06_moderncnn/06-02-dilated/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/06_moderncnn/06-02-dilated/</guid><description>&lt;p>This subchapter explores dilated and transposed convolutions, specialized types of convolutional operations in CNNs. Dilated convolutions expand the receptive field without increasing the number of parameters, whereas transposed convolutions upsample data by increasing feature map dimensions, often used in applications requiring higher resolution outputs.&lt;/p></description></item><item><title>Chapter 06.03: Separable Convolutions and Flattening</title><link>https://slds-lmu.github.io/i2dl/chapters/06_moderncnn/06-03-separable/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/06_moderncnn/06-03-separable/</guid><description>&lt;p>This subsection introduces separable convolutions and the flattening process in CNNs. Separable convolutions improve computational efficiency, whilst flattening converts multidimensional feature maps into a 1D array i.a. bridging the gap between convolutional and dense layers.&lt;/p></description></item><item><title>Chapter 06.04: Modern Convolutional Architectures - Part I</title><link>https://slds-lmu.github.io/i2dl/chapters/06_moderncnn/06-04-arch1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/06_moderncnn/06-04-arch1/</guid><description>&lt;p>In this subsection, we explain the recent popular CNN-based architectures such as LeNet, AlexNet and VGG.&lt;/p></description></item><item><title>Chapter 06.05: Modern Convolutional Architectures - Part II</title><link>https://slds-lmu.github.io/i2dl/chapters/06_moderncnn/06-05-arch2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/06_moderncnn/06-05-arch2/</guid><description>&lt;p>In this subsection, we focus further on modern CNN architectures such as GoogleNet, ResNet and DenseNet.&lt;/p></description></item></channel></rss>
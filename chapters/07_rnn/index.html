<!DOCTYPE html>
<html><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="/i2dl/css/style.css">


<title>Introduction to Machine Learning (I2ML) | Topic 07: Deep Recurrent Neural Networks</title>


<link rel="apple-touch-icon" sizes="180x180" href="/i2dl/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/i2dl/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/i2dl/favicon-16x16.png">
<link rel="manifest" href="/i2dl/site.webmanifest">
<link rel="mask-icon" href="/i2dl/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">

</head><body>
<img id="logo" src="/i2dl/i2ml.svg" />

<div id="nav-border" class="container">
    <nav id="nav" class="nav justify-content-center">
        
        <a class="nav-link" href="/i2dl">
        
        Home
        </a>
        
        <a class="nav-link" href="/i2dl/chapters/">
        
        Chapters
        </a>
        
        <a class="nav-link" href="/i2dl/exercises/">
        
        Exercises
        </a>
        
        <a class="nav-link" href="/i2dl/appendix/">
        
        Appendix
        </a>
        
        <a class="nav-link" href="/i2dl/prerequisites/">
        
        Prerequisites
        </a>
        
        <a class="nav-link" href="/i2dl/literature/">
        
        Literature
        </a>
        
        <a class="nav-link" href="/i2dl/team/">
        
        Team
        </a>
        
        <a class="nav-link" href="/i2dl/news/">
        
        News
        </a>
        
    </nav>
</div><div id="content" class="container">
<h1>Topic 07: Deep Recurrent Neural Networks</h1>

<p><p>This chapter introduces Recurrent Neural Networks (RNNs), designed to process sequential data by retaining information over time. It covers the backpropagation through time (BPTT) algorithm for training RNNs, highlighting key challenges like exploding and vanishing gradients. To address these issues, Long Short-Term Memory (LSTM) networks and Gated Recurrent Units (GRU) are introduced as enhanced architectures with gating mechanisms that better manage information flow. In addition, the chapter briefly introduces more recent approaches for modelling sequence data such as attention and transformers.</p>
</p>


<div class="chapter_overview">
<ul class="list-unstyled">


<li>
    <a class="title" href="/i2dl/chapters/07_rnn/07-01-intro-rnn/">Chapter 07.01: Introduction to RNNs</a>
    
      
        <p>Recurrent neural networks (RNN) are the state of the art algorithm for sequential data and are used by Apple&rsquo;s Siri and Google&rsquo;s voice search. It is the first algorithm that remembers its input, due to an internal memory, which makes it perfectly suited for machine learning problems that involve sequential data.
</p>
      
      
</li>

<li>
    <a class="title" href="/i2dl/chapters/07_rnn/07-02-backprob-rnn/">Chapter 07.02: Backpropogation in RNNs</a>
    
      
        <p>In this section we explain backpropagation for RNN as well as the phenomenon of exploding and vanishing gradients.
</p>
      
      
</li>

<li>
    <a class="title" href="/i2dl/chapters/07_rnn/07-03-modern-rnn/">Chapter 07.03: Modern RNNs</a>
    
      
        <p>In this subchapter, we explain how modern RNN such as LSTM, GRU, and Bidirectional RNNs address the problem of exploding and vanishing gradients.
</p>
      
      
</li>

<li>
    <a class="title" href="/i2dl/chapters/07_rnn/07-04-applics-rnn/">Chapter 07.04: Applications of RNNs</a>
    
      
        <p>This subsection focuses on common applications of RNNs, e.g. in the context of large language modelling or encoder-decoder architectures.
</p>
      
      
</li>

<li>
    <a class="title" href="/i2dl/chapters/07_rnn/07-05-attention-transf-rnn/">Chapter 07.05: Attention and Transformers</a>
    
      
        <p>In this subchapter, we introduce more recent sequence data modelling techniques such as attention and transformers.
</p>
      
      
</li>


</ul>
</div>


        </div><footer class="bg-light text-center text-lg-start fixed-bottom">
<ul class="list-inline text-center">
  <li class="list-inline-item">Â© 2022 Course Creator</li>
  
  <li class="list-inline-item"><a class="nav-link" href="/i2dl" target="_blank">Main Course Website</a></li>
  
  <li class="list-inline-item"><a class="nav-link" href="https://github.com/slds-lmu/lecture_i2dl" target="_blank">Material Source Code</a></li>
  
  <li class="list-inline-item"><a class="nav-link" href="https://github.com/slds-lmu/i2dl" target="_blank">Website Source Code</a></li>
  
</ul>
</footer>

<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      ignoreHtmlClass: ['quizdown']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</body>
</html>

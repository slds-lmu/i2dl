<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Topic 07: Deep Recurrent Neural Networks on Introduction to Machine Learning (I2ML)</title><link>https://slds-lmu.github.io/i2dl/chapters/07_rnn/</link><description>Recent content in Topic 07: Deep Recurrent Neural Networks on Introduction to Machine Learning (I2ML)</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://slds-lmu.github.io/i2dl/chapters/07_rnn/index.xml" rel="self" type="application/rss+xml"/><item><title>Chapter 07.01: Recurrent Neural Networks - Introduction</title><link>https://slds-lmu.github.io/i2dl/chapters/07_rnn/07-01-intro-rnn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/07_rnn/07-01-intro-rnn/</guid><description>&lt;p>Recurrent neural networks (RNN) are the state of the art algorithm for sequential data and are used by Apple&amp;rsquo;s Siri and Google&amp;rsquo;s voice search. It is the first algorithm that remembers its input, due to an internal memory, which makes it perfectly suited for machine learning problems that involve sequential data.&lt;/p></description></item><item><title>Chapter 07.02: Recurrent Neural Networks - Backpropogation</title><link>https://slds-lmu.github.io/i2dl/chapters/07_rnn/07-02-backprob-rnn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/07_rnn/07-02-backprob-rnn/</guid><description>&lt;p>In this section we explain the backpropagation for RNN as well as exploiding and vanishing gradients.&lt;/p></description></item><item><title>Chapter 07.03: Modern Recurrent Neural Networks</title><link>https://slds-lmu.github.io/i2dl/chapters/07_rnn/07-03-modern-rnn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/07_rnn/07-03-modern-rnn/</guid><description>&lt;p>We explain how modern RNN such as LSTM, GRU, and Bidirectional RNN addressed problem of exploding and vanishing gradient by conventional RNN.&lt;/p></description></item></channel></rss>
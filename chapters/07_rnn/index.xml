<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Topic 07: Deep Recurrent Neural Networks on Introduction to Machine Learning (I2ML)</title><link>https://slds-lmu.github.io/i2dl/chapters/07_rnn/</link><description>Recent content in Topic 07: Deep Recurrent Neural Networks on Introduction to Machine Learning (I2ML)</description><generator>Hugo</generator><language>en-us</language><atom:link href="https://slds-lmu.github.io/i2dl/chapters/07_rnn/index.xml" rel="self" type="application/rss+xml"/><item><title>Chapter 07.01: Introduction to RNNs</title><link>https://slds-lmu.github.io/i2dl/chapters/07_rnn/07-01-intro-rnn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/07_rnn/07-01-intro-rnn/</guid><description>&lt;p>Recurrent neural networks (RNN) are the state of the art algorithm for sequential data and are used by Apple&amp;rsquo;s Siri and Google&amp;rsquo;s voice search. It is the first algorithm that remembers its input, due to an internal memory, which makes it perfectly suited for machine learning problems that involve sequential data.&lt;/p></description></item><item><title>Chapter 07.02: Backpropogation in RNNs</title><link>https://slds-lmu.github.io/i2dl/chapters/07_rnn/07-02-backprob-rnn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/07_rnn/07-02-backprob-rnn/</guid><description>&lt;p>In this section we explain backpropagation for RNN as well as the phenomenon of exploding and vanishing gradients.&lt;/p></description></item><item><title>Chapter 07.03: Modern RNNs</title><link>https://slds-lmu.github.io/i2dl/chapters/07_rnn/07-03-modern-rnn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/07_rnn/07-03-modern-rnn/</guid><description>&lt;p>In this subchapter, we explain how modern RNN such as LSTM, GRU, and Bidirectional RNNs address the problem of exploding and vanishing gradients.&lt;/p></description></item><item><title>Chapter 07.04: Applications of RNNs</title><link>https://slds-lmu.github.io/i2dl/chapters/07_rnn/07-04-applics-rnn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/07_rnn/07-04-applics-rnn/</guid><description>&lt;p>This subsection focuses on common applications of RNNs, e.g. in the context of large language modelling or encoder-decoder architectures.&lt;/p></description></item><item><title>Chapter 07.05: Attention and Transformers</title><link>https://slds-lmu.github.io/i2dl/chapters/07_rnn/07-05-attention-transf-rnn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/07_rnn/07-05-attention-transf-rnn/</guid><description>&lt;p>In this subchapter, we introduce more recent sequence data modelling techniques such as attention and transformers.&lt;/p></description></item></channel></rss>
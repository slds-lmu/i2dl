<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Chapter 08: Tuning on Introduction to Machine Learning (I2ML)</title><link>https://slds-lmu.github.io/i2dl/chapters/08_modernrnn/</link><description>Recent content in Chapter 08: Tuning on Introduction to Machine Learning (I2ML)</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://slds-lmu.github.io/i2dl/chapters/08_modernrnn/index.xml" rel="self" type="application/rss+xml"/><item><title>Chapter 08.01: Introduction</title><link>https://slds-lmu.github.io/i2dl/chapters/08_modernrnn/08-01-intro/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/08_modernrnn/08-01-intro/</guid><description>&lt;p>While model parameters are optimized during training, hyperparameters must be specified in advance. In this section, we will motivate why it is crucial to find good values for, i.e. to tune, these hyperparameters.&lt;/p></description></item><item><title>Chapter 08.02: Problem Definition</title><link>https://slds-lmu.github.io/i2dl/chapters/08_modernrnn/08-02-tuning-tuningproblem/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/08_modernrnn/08-02-tuning-tuningproblem/</guid><description>&lt;p>Hyperparameter tuning is the process of finding good model hyperparameters. In this section we formalize the problem of tuning and explain why tuning is computationally hard.&lt;/p></description></item><item><title>Chapter 08.03: Basic Techniques</title><link>https://slds-lmu.github.io/i2dl/chapters/08_modernrnn/08-03-basicalgos/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/08_modernrnn/08-03-basicalgos/</guid><description>&lt;p>In this section we familiarize ourselves with two simple but popular tuning strategies, namely grid search and random search, and discuss their advantages and disadvantages.&lt;/p></description></item><item><title>Chapter 08.04: Advanced Tuning Techniques</title><link>https://slds-lmu.github.io/i2dl/chapters/08_modernrnn/08-04-tuning-advanced/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/08_modernrnn/08-04-tuning-advanced/</guid><description>&lt;p>Besides grid search and random search there are several more advanced techniques for hyperparameter optimization. In this section we focus on model based optimization methods such as Bayesian optimization. Furthermore, we look into multi-fidelity methods such as the hyperband algorithm.&lt;/p></description></item><item><title>Chapter 08.05: Pipelines and AutoML</title><link>https://slds-lmu.github.io/i2dl/chapters/08_modernrnn/08-05-tuning-pipelines/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/08_modernrnn/08-05-tuning-pipelines/</guid><description>&lt;p>Some aspects of the machine learning lifecycle can be automated via
AutoML. In this section we look into pipelines as part of AutoML and how (HPO-) pipelines can be represented as directed acyclic graphs (DAGs).&lt;/p></description></item></channel></rss>
<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Chapter 13: Information Theory on Introduction to Machine Learning (I2ML)</title><link>https://slds-lmu.github.io/i2dl/chapters/13_information_theory/</link><description>Recent content in Chapter 13: Information Theory on Introduction to Machine Learning (I2ML)</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://slds-lmu.github.io/i2dl/chapters/13_information_theory/index.xml" rel="self" type="application/rss+xml"/><item><title>Chapter 13.01: Entropy</title><link>https://slds-lmu.github.io/i2dl/chapters/13_information_theory/13-01-entropy/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/13_information_theory/13-01-entropy/</guid><description>&lt;p>We introduce entropy, which expresses the expected information for discrete random variables, as a central concept in information theory.&lt;/p></description></item><item><title>Chapter 13.02: Differential Entropy</title><link>https://slds-lmu.github.io/i2dl/chapters/13_information_theory/13-02-diffent/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/13_information_theory/13-02-diffent/</guid><description>&lt;p>In this section, we extend the definition of entropy to the continuous case.&lt;/p></description></item><item><title>Chapter 13.03: Kullback-Leibler Divergence</title><link>https://slds-lmu.github.io/i2dl/chapters/13_information_theory/13-03-kl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/13_information_theory/13-03-kl/</guid><description>&lt;p>The Kullback-Leibler divergence (KL) is an important quantity for measuring the difference between two probability distributions. We discuss different intuitions for KL and relate it to risk minimization and likelihood ratios.&lt;/p></description></item><item><title>Chapter 13.04: Entropy and Optimal Code Length</title><link>https://slds-lmu.github.io/i2dl/chapters/13_information_theory/13-04-sourcecoding/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/13_information_theory/13-04-sourcecoding/</guid><description>&lt;p>In this section, we introduce source coding and discuss how entropy can be understood as optimal code length.&lt;/p></description></item><item><title>Chapter 13.05: Cross-Entropy, KL and Source Coding</title><link>https://slds-lmu.github.io/i2dl/chapters/13_information_theory/13-05-cross-entropy-kld/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/13_information_theory/13-05-cross-entropy-kld/</guid><description>&lt;p>We introduce cross-entropy as a further information-theoretic concept and discuss the connection between entropy, cross-entropy, and Kullback-Leibler divergence.&lt;/p></description></item><item><title>Chapter 13.06: Information Theory for Machine Learning</title><link>https://slds-lmu.github.io/i2dl/chapters/13_information_theory/13-06-ml/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/13_information_theory/13-06-ml/</guid><description>&lt;p>In this section, we discuss how information-theoretic concepts are used in machine learning and demonstrate the equivalence of KL minimization and maximum likelihood maximization, as well as how (cross-)entropy can be used as a loss function.&lt;/p></description></item><item><title>Chapter 13.07: Joint Entropy and Mutual Information</title><link>https://slds-lmu.github.io/i2dl/chapters/13_information_theory/13-07-mutual-info/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/13_information_theory/13-07-mutual-info/</guid><description>&lt;p>Information theory also provides means of quantifying relations between two random variables that extend the concept of (linear) correlation. We discuss joint entropy, conditional entropy, and mutual information in this context.&lt;/p></description></item></channel></rss>
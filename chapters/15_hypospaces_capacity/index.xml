<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Chapter 15: Hypothesis Spaces on Introduction to Machine Learning (I2ML)</title><link>https://slds-lmu.github.io/i2dl/chapters/15_hypospaces_capacity/</link><description>Recent content in Chapter 15: Hypothesis Spaces on Introduction to Machine Learning (I2ML)</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://slds-lmu.github.io/i2dl/chapters/15_hypospaces_capacity/index.xml" rel="self" type="application/rss+xml"/><item><title>Chapter 15.01: Examples of Hypothesis Spaces</title><link>https://slds-lmu.github.io/i2dl/chapters/15_hypospaces_capacity/15-01-hypospaces-examples/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/15_hypospaces_capacity/15-01-hypospaces-examples/</guid><description>&lt;p>In this section, we show examples for formal definitions of hypothesis spaces.&lt;/p></description></item><item><title>Chapter 15.02: Capacity and Overfitting</title><link>https://slds-lmu.github.io/i2dl/chapters/15_hypospaces_capacity/15-02-hypospaces-capacity-overfitting/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/15_hypospaces_capacity/15-02-hypospaces-capacity-overfitting/</guid><description>&lt;p>In this section, we discuss how the capacity of a hypothesis influences the overfitting behavior of a learner.&lt;/p></description></item><item><title>Chapter 15.03: PAC Learning and VC Dimension</title><link>https://slds-lmu.github.io/i2dl/chapters/15_hypospaces_capacity/15-03-complexity/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/15_hypospaces_capacity/15-03-complexity/</guid><description>&lt;p>The &amp;ldquo;no free lunch&amp;rdquo; theorem implies that we cannot construct a universally optimal learner. More precisely, only finite hypothesis spaces, as measured by Vapnik-Chervonenkis (VC) dimension, are agnostic probably-approximately-correct (PAC) learnable, which we will discuss in this section.&lt;/p></description></item><item><title>Chapter 15.04: Bias-Variance Decomposition</title><link>https://slds-lmu.github.io/i2dl/chapters/15_hypospaces_capacity/15-04-bias-variance-decomposition/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/15_hypospaces_capacity/15-04-bias-variance-decomposition/</guid><description>&lt;p>In this section, we derive and discuss the classic bias-variance decomposition of the generalization error of an inducer.&lt;/p></description></item></channel></rss>
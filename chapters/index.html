<!DOCTYPE html>
<html><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="/i2dl/css/style.css">


<title>Introduction to Machine Learning (I2ML) | Chapters</title>


<link rel="apple-touch-icon" sizes="180x180" href="/i2dl/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/i2dl/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/i2dl/favicon-16x16.png">
<link rel="manifest" href="/i2dl/site.webmanifest">
<link rel="mask-icon" href="/i2dl/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">

</head><body>
<img id="logo" src="/i2dl/i2ml.svg" />

<div id="nav-border" class="container">
    <nav id="nav" class="nav justify-content-center">
        
        <a class="nav-link" href="/i2dl">
        
        Home
        </a>
        
        <a class="nav-link" href="/i2dl/chapters/">
        
        Chapters
        </a>
        
        <a class="nav-link" href="/i2dl/exercises/">
        
        Exercises
        </a>
        
        <a class="nav-link" href="/i2dl/appendix/">
        
        Appendix
        </a>
        
        <a class="nav-link" href="/i2dl/prerequisites/">
        
        Prerequisites
        </a>
        
        <a class="nav-link" href="/i2dl/literature/">
        
        Literature
        </a>
        
        <a class="nav-link" href="/i2dl/team/">
        
        Team
        </a>
        
        <a class="nav-link" href="/i2dl/news/">
        
        News
        </a>
        
    </nav>
</div><div id="content" class="container">
<h1>Chapters</h1>

<p></p>


<div class="chapter_overview">
<ul class="list-unstyled">


<li>
    <a class="title" href="/i2dl/chapters/01_ml_basics/">Chapter 1: ML Basics</a>
    
      
        <p>This chapter introduces the basic concepts of Machine Learning. We focus on supervised learning, explain the difference between regression and classification, show how to evaluate and compare Machine Learning models and formalize the concept of learning.</p>
      
      
</li>

<li>
    <a class="title" href="/i2dl/chapters/02_supervised_regression/">Chapter 2: Supervised Regression</a>
    
      
        <p>This chapter treats the supervised regression task in more detail. We will see different loss functions for regression, how a linear regression model can be used from a Machine Learning perspective, and how to extend it with polynomials for greater flexibility.</p>
      
      
</li>

<li>
    <a class="title" href="/i2dl/chapters/03_supervised_classification/">Chapter 03: Supervised Classification</a>
    
      
        <p>This chapter treats the supervised classification task in more detail. We will see examples of binary and multiclass classification and the differences between discriminative and generative approaches. In particular, we will address logistic regression, discriminant analysis and naive Bayes classifiers.</p>
      
      
</li>

<li>
    <a class="title" href="/i2dl/chapters/04_evaluation/">Chapter 04: Performance Evaluation</a>
    
      
        <p>This chapter treats the challenge of evaluating the performance of a model. We will introduce different performance measures for regression and classification tasks, explain the problem of overfitting as well as the difference between training and test error, and, lastly, present a variety of resampling techniques.</p>
      
      
</li>

<li>
    <a class="title" href="/i2dl/chapters/05_knn/">Chapter 05: k-Nearest Neighbors (k-NN)</a>
    
      
        <p>This chapter addresses \(k\)-nearest neighbors, a distance-based algorithm suited to both regression and classification. Predictions are made based upon neighboring observations, assuming feature similarity translates to target similarity.</p>
      
      
</li>

<li>
    <a class="title" href="/i2dl/chapters/06_trees/">Chapter 06: Classification and Regression Trees (CART)</a>
    
      
        <p>This chapter introduces Classification and Regression Trees (CART), a well-established machine learning procedure. We explain the main idea and give details on splitting criteria, discuss computational aspects of growing a tree, and illustrate the idea of stopping criteria and pruning.</p>
      
      
</li>

<li>
    <a class="title" href="/i2dl/chapters/07_forests/">Chapter 07: Random Forests</a>
    
      
        <p>This chapter introduces bagging as a method to increase the performance of trees (or other base learners). A modification of bagging leads to random forests. We explain the main idea of random forests, benchmark their performance with the methods seen so far and show how to quantify the impact of a single feature on the performance of the random forest as well as how to compute proximities between observations.</p>
      
      
</li>

<li>
    <a class="title" href="/i2dl/chapters/08_tuning/">Chapter 08: Tuning</a>
    
      
        <p>This chapter introduces and formalizes the problem of hyperparameter tuning. We cover basic techniques such as grid search and random search as well as more advanced techniques like evolutionary algorithms, model-based optimization and multi-fidelity optimization.</p>
      
      
</li>

<li>
    <a class="title" href="/i2dl/chapters/09_nested_resampling/">Chapter 09: Nested Resampling</a>
    
      
        <p>This chapter first defines the untouched-test-set principle and proceeds to explain the concepts of train-validation-test split and nested resampling.</p>
      
      
</li>

<li>
    <a class="title" href="/i2dl/chapters/10_mlr3/">Chapter 10: mlr3</a>
    
      
        <p>This chapter introduces the R package mlr3. After some basic concepts we focus on resampling, tuning and pipelines.</p>
      
      
</li>

<li>
    <a class="title" href="/i2dl/chapters/11_advriskmin/">Chapter 11: Advanced Risk Minimization</a>
    
      
        <p>This chapter revisits the theory of risk minimization, providing more in-depth analysis on established losses and the connection between empirical risk minimization and maximum likelihood estimation. We also introduce some more advanced loss functions for regression and classification.</p>
      
      
</li>

<li>
    <a class="title" href="/i2dl/chapters/12_multiclass/">Chapter 12: Multiclass Classification</a>
    
      
        <p>This chapter treats the multiclass case of classification. Tasks with more than two classes preclude the application of some techniques studied in the binary scenario and require an adaptation of loss functions.</p>
      
      
</li>

<li>
    <a class="title" href="/i2dl/chapters/13_information_theory/">Chapter 13: Information Theory</a>
    
      
        <p>This chapter covers basic information-theoretic concepts and discusses their relation to machine learning.</p>
      
      
</li>

<li>
    <a class="title" href="/i2dl/chapters/14_cod/">Chapter 14: Curse of Dimensionality</a>
    
      
        <p>Frequently, our intuition developed in low-dimensional spaces does not generalize to higher dimensions. This chapter introduces the phenomenon of the curse of dimensionality and discusses its effects on the behavior of machine learning models.</p>
      
      
</li>

<li>
    <a class="title" href="/i2dl/chapters/15_hypospaces_capacity/">Chapter 15: Hypothesis Spaces</a>
    
      
        <p>Machine learning constantly balances model fit and generalization ability. This chapter discusses the capacity of hypothesis spaces in more depth, also touching upon Vapnik-Chervonenkis (VC) dimensions and probably approximately correct (PAC) learning.</p>
      
      
</li>

<li>
    <a class="title" href="/i2dl/chapters/16_regularization/">Chapter 16: Regularization</a>
    
      
        <p>Regularization is a vital tool in machine learning to prevent overfitting and foster generalization ability. This chapter introduces the concept of regularization and discusses common regularization techniques in more depth.</p>
      
      
</li>

<li>
    <a class="title" href="/i2dl/chapters/17_linear_svm/">Chapter 17: Linear Support Vector Machines</a>
    
      
        <p>This chapter introduces the linear support vector machine (SVM), a linear classifier that finds decision boundaries by maximizing margins to the closest data points, possibly allowing for violations to a certain extent.</p>
      
      
</li>

<li>
    <a class="title" href="/i2dl/chapters/18_nonlinear_svm/">Chapter 18: Nonlinear Support Vector Machines</a>
    
      
        <p>Many classification problems warrant nonlinear decision boundaries. This chapter introduces nonlinear support vector machines as a crucial extension to the linear variant.</p>
      
      
</li>

<li>
    <a class="title" href="/i2dl/chapters/19_gaussian_processes/">Chapter 19: Gaussian Processes</a>
    
      
        <p>This chapter introduces Gaussian processes as a model class. Gaussian processes are non-parametric approaches with ubiquitous application that model entire distributions in function space.</p>
      
      
</li>

<li>
    <a class="title" href="/i2dl/chapters/20_boosting/">Chapter 20: Boosting</a>
    
      
        <p>This chapter introduces boosting as a sequential ensemble method that creates powerful committees from different kinds of base learners.</p>
      
      
</li>


</ul>
</div>


        </div><footer class="bg-light text-center text-lg-start fixed-bottom">
<ul class="list-inline text-center">
  <li class="list-inline-item">Â© 2022 Course Creator</li>
  
  <li class="list-inline-item"><a class="nav-link" href="/i2dl" target="_blank">Main Course Website</a></li>
  
  <li class="list-inline-item"><a class="nav-link" href="https://github.com/slds-lmu/lecture_i2dl" target="_blank">Material Source Code</a></li>
  
  <li class="list-inline-item"><a class="nav-link" href="https://github.com/slds-lmu/i2dl" target="_blank">Website Source Code</a></li>
  
</ul>
</footer>

<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      ignoreHtmlClass: ['quizdown']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</body>
</html>

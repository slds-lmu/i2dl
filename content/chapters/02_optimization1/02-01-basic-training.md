---
title: "Chapter 02.01: Basic Training"
weight: 2001
---
This subchapter covers essential principles of neural network training, starting with empirical risk minimization (ERM) and gradient descent (GD). Additionally, it introduces stochastic gradient descent (SGD) as a computationally efficient alternative to GD.
<!--more-->
### Lecture slides

{{< pdfjs file="https://github.com/slds-lmu/lecture_i2dl/blob/main/slides-pdf/opt1/slides-opt1-basic-training.pdf" >}}




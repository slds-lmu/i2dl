---
title: "Chapter 05.01: k-Nearest Neighbors (k-NN)"
weight: 5001
quizdown: true
---
We demonstrate that distances in feature space are crucial in \\(k\\)-NN regression / classification and show how we can form predictions by averaging / majority vote. In this, \\(k\\)-NN is a very local model and works without distributional assumptions.

<!--more-->

### Lecture video

{{< video id="BMCgd1et_2E">}}

### Lecture slides

{{< pdfjs file="https://github.com/slds-lmu/lecture_i2ml/tree/master/slides-pdf/slides-knn.pdf" >}}

### Quiz

{{< quizdown >}}

---
shuffle_questions: false
---

## Which statements are true? 

- [x] Choosing the distance metric is a crucial design decision for $k$-NN.
- [ ] $k$-NN can only be used for classification tasks.
- [x] $N_k(x)$ contains the subset of the feature space $\mathcal{X}$ that is at least as close to $x$ as the $k$-th closest neighbor of $x$ in the training data set.
- [x] 1-NN always 'predicts' perfectly on observations of the training data set (if there are no observations with equal feature but different target values).
- [x] $k$-NN with $k = n$ always predicts the same target variable value for all possible inputs (if no weights are used).
- [ ] $k$-NN for classification is a probabilistic classifier.

{{< /quizdown >}}
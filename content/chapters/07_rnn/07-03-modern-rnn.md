---
title: "Chapter 07.03: Modern Recurrent Neural Networks"
weight: 7003
---
We explain how modern RNN such as LSTM, GRU, and Bidirectional RNN addressed problem of exploding and vanishing gradient by conventional RNN.   

<!--more-->

### Lecture video

[video](https://drive.google.com/file/d/1eSrsGhMvivSinIpk4izKROBOJE8rHIEd/view?usp=sharing)

### Lecture slides

{{< pdfjs file="https://github.com/slds-lmu/lecture_i2dl/blob/main/slides-pdf/topic7/slides-8-3-modernrnn.pdf" >}}


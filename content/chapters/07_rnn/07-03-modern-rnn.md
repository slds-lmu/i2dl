---
title: "Chapter 07.03: Modern Recurrent Neural Networks"
weight: 7003
---
We explain how modern RNN such as LSTM, GRU, and Bidirectional RNN addressed problem of exploding and vanishing gradient by conventional RNN.   

<!--more-->

### Lecture video

{{< video id="" >}}

### Lecture slides

{{< pdfjs file="https://github.com/slds-lmu/lecture_i2dl/blob/main/slides-pdf/slides-8-3-modernrnn.pdf" >}}


---
title: "Chapter 07.05: Attention and Transformers"
weight: 7005
---
In this subchapter, we introduce more recent sequence data modelling techniques such as attention and transformers.

<!--more-->
### Lecture slides

{{< pdfjs file="https://github.com/slds-lmu/lecture_i2dl/blob/main/slides-pdf/rnn/slides-attention.pdf" >}}


---
title: "Chapter 13.03: Kullback-Leibler Divergence"
weight: 13003
---
The Kullback-Leibler divergence (KL) is an important quantity for measuring the difference between two probability distributions. We discuss different intuitions for KL and relate it to risk minimization and likelihood ratios. 

<!--more-->

### Lecture video

{{< video id="kC0XXQgC4_k" >}}

### Lecture slides

{{< pdfjs file="https://github.com/slds-lmu/lecture_i2ml/tree/master/slides-pdf/slides-info-kl.pdf" >}}

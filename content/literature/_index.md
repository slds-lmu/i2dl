---
title: Literature
---

The course material covers all exam-relevant topics in a quite self-contained manner. 
For more in-depth study, we recommend the following literature. Note that some of the books are rather detailed and involved, and more geared towards a larger lecture in a Master's degree. 

We **recommend to buy and read** at least one standard reference on ML, for BSc level this might be the James, for the MSc level the Hastie, Bishop, Murphy or Alplaydin, the Shalev-Shwartz for a mathematical entry point.

## Helpful References for Prerequisites

If you need to read up on some of the required topics (see [Prerequisites](../prerequisites)), this list might help. We tried to keep it as short as possible. 
- M. Deisenroth, A. Faisal, C. Ong. Mathematics for Machine Learning. [URL](https://mml-book.github.io/book/mml-book.pdf)
- L. Wassermann. All of Statistics. [URL](http://egrcc.github.io/docs/math/all-of-statistics.pdf)
- H. Wickham, G. Grolemund. R for Data Science. [URL](https://r4ds.had.co.nz/)
- Introductory R course on datacamp.com [URL](https://learn.datacamp.com/courses/free-introduction-to-r)


## Machine Learning

- K. Kersting, C. Lampert, C. Rothkopf. Wie Maschinen Lernen. Springer, 2019. [URL](https://link.springer.com/book/10.1007/978-3-658-26763-6) *German, informal, intuitive introduction to ML. Lower than BSc level, maybe more targeted at pupils or a non-academic audience. Read if you want a very light-weight introduction into the field, or buy as present for relatives and friends if they ask what you are doing.*
- G. James, D. Witten, T. Hastie, R. Tibshirani. An Introduction to Statistical Learning. MIT Press, 2010. [URL](http://www-bcf.usc.edu/~gareth/ISL/) *Beginner-level introduction with applications in R. Very well suited for the BSc level.*
- T. Hastie, R. Tibshirani, J. Friedman. The Elements of Statistical Learning. Springer, 2009. [URL](https://web.stanford.edu/~hastie/ElemStatLearn/) *Standard reference for statistics-flavored ML.*
- C. M. Bishop. Pattern Recognition and Machine Learning. Springer, 2006. [URL](http://research.microsoft.com/en-us/um/people/cmbishop/prml/) *Standard reference for ML-flavored ML.*
- S. Shalev-Shwartz, S. Ben-David. Understanding machine learning: From Theory to Algorithms. Cambridge University Press, 2014. [URL](https://www.cs.huji.ac.il/w~shais/UnderstandingMachineLearning/) *Great, thorough introduction to ML theory. Math-style book with definitions and proofs.*
- E. Alpaydin. Introduction to Machine Learning. MIT Press, 2010. [URL](http://www.cmpe.boun.edu.tr/~ethem/i2ml2e/) *Standard reference with broad coverage; easy to read.*
- K. Murphy. Machine Learning: a Probabilistic Perspective. MIT Press, 2012. [URL](https://probml.github.io/pml-book/book0.html) *Standard reference; quite extensive; statistical/probabilistic lens.*
- F. Provost, T. Fawcett. Data Science for Business. Oâ€™Reilly, 2013. [URL](https://book.akij.net/eBooks/2018/May/5aef50939a868/Data_Science_for_Bus.pdf) *A very good, applied and easy-to-read book by 2 well-known ML scientists. Contains many practical aspects that are missing in other references. Probably a good idea to read this in any case.*
- N. Japkowicz. Evaluating Learning Algorithms (A Classification Perspective). Cambridge University Press, 2011. *Nice reading on performance measures, resampling methods and (some) statistical tests for benchmarking in ML; only for classification.*
- B. Bischl et al. Hyperparameter Optimization: Foundations, Algorithms, Best Practices and Open Challenges. arXiv preprint 2021. [URL](https://arxiv.org/pdf/2107.05847.pdf) *Our tutorial paper on HPO.*
- I. Goodfellow, Y. Bengio, A. Courville. Deep Learning. MIT Press, 2016. [URL](https://www.deeplearningbook.org/) *Standard, modern reference for DL.*

## Mathematical & Statistical Theory

- G. Strang. Linear Algebra and Learning from Data. Cambridge University Press, 2019. *Serious course on matrices and applied linear algebra.*
- S. Axler. Linear Algebra Done Right. Springer, 2015. [URL](https://link.springer.com/content/pdf/10.1007%2F978-3-319-11080-6.pdf) *Linear Algebra from a more theoretical but still beginner-friendly perspective*
- A. M. Mood, F. A. Graybill, D. C. Boes. Introduction to the Theory of Statistics, McGraw-Hill 1974. [URL](https://www.fulviofrisone.com/attachments/article/446/Introduction%20to%20the%20theory%20of%20statistics%20by%20MOOD.pdf) *Beginner-friendly intro to statistics; bit on the mathy side.*
- J. Watt, R. Borhani, A. Katsaggelos. Machine Learning Refined. Cambridge University Press, 2020. [URL](https://github.com/jermwatt/machine_learning_refined) *Check chapters 2-4 plus Appendix for insightful explanations and visualizations of a variety of optimization concepts.*
- T. M. Cover, J. A. Thomas. Elements of Information Theory. Wiley, 2006. [URL](http://staff.ustc.edu.cn/~cgong821/Wiley.Interscience.Elements.of.Information.Theory.Jul.2006.eBook-DDU.pdf) *Good intro to information theory in first hundred pages, though lacking cross-connections to ML / statistics.*

## R Programming

- N. Matloff. The Art of R Programming. No Starch Press, 2011. [URL](https://diytranscriptomics.com/Reading/files/The%20Art%20of%20R%20Programming.pdf)

We use the **mlr3** package for machine learning in R quite heavily.
- Central project page and learning resources: https://mlr3.mlr-org.com/, in particular
  - the [book](https://mlr3book.mlr-org.com/),
  - the [gallery](https://mlr3gallery.mlr-org.com/), and   
  - the [cheatsheets](https://cheatsheets.mlr-org.com/).
- GitHub page: https://github.com/mlr-org/mlr3

---
title: Literature
---

The course material covers all exam-relevant topics in a quite self-contained manner. 
For more in-depth study, we recommend the following literature. Note that some of the books are rather detailed and involved, and more geared towards a larger lecture in a Master's degree. 

We **recommend to buy and read** at least one standard reference on ML, for BSc level this might be the James, for the MSc level the Hastie, Bishop, Murphy or Alplaydin, the Shalev-Shwartz for a mathematical entry point, and Deep Learning book written by Goodfelow, d2dl.ai for a python and DL entry.

## Helpful References for Prerequisites

If you need to read up on some of the required topics (see [Prerequisites](../prerequisites)), this list might help. We tried to keep it as short as possible. 

- [ Alex J. Smola (2020): *Dive into Deep Learning*](https://d2l.ai/index.html) (An interactive deep learning book with code, math, and discussions Provides NumPy/MXNet, PyTorch, and TensorFlow implementations)(free HTML version)
- [Goodfellow, Bengio, Courville (2016): *Deep Learning*](http://www.deeplearningbook.org/) (free HTML version)
- [Awesome Deep Learning](https://github.com/ChristosChristofidis/awesome-deep-learning)
- [Andrej Karpathy blog](http://karpathy.github.io/)
- [Coursera Kurs "Neural Networks for Machine Learning"](https://www.coursera.org/learn/neural-networks#syllabus)
- [Youtube Talk von Geoff Hinton "Recent Developments in Deep Learning"](https://www.youtube.com/watch?v=vShMxxqtDDs)
- [Practical Deep Learning For Coders - contains many detailed python notebooks on how to implement different DL architectures](http://course.fast.ai/index.html)
- [The Matrix Calculus You Need For Deep Learning](http://parrt.cs.usfca.edu/doc/matrix-calculus/index.html) 
- [Tensorflow Neural Network Playground](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.14139&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)
- [A Weird Introduction to Deep Learning](https://towardsdatascience.com/a-weird-introduction-to-deep-learning-7828803693b0)
- [Deep Learning Achievements Over the Past Year](https://blog.statsbot.co/deep-learning-achievements-4c563e034257)
- [Scalable Deep Learning (Talk)](https://determined.ai/blog/talk-scalable-dl/)
- [Deep Learning Resources](https://sebastianraschka.com/deep-learning-resources.html)

## Good Websites to have a look 
 - [distill.pub](https://distill.pub/): in-depth explanations of important concepts, worth checking out periodically for new material


## Optimization / Training of NNs:

 - [Why Momentum Really Works](https://distill.pub/2017/momentum/)
 - [Adam -- latest trends in deep learning optimization](https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6be9a291375c)
 - [Overview of Gradient Descent Optimization Algorithms](https://ruder.io/optimizing-gradient-descent/)
 - [Yes you should understand backprop](https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b)
 - [A Recipe for Training Neural Networks](https://karpathy.github.io/2019/04/25/recipe/)

## Regularization:

- [Regularization for Deep Learning: A Taxonomy](https://arxiv.org/pdf/1710.10686.pdf)

## CNNs:

- [The Sobel and Laplacian Edge Detectors](http://aishack.in/tutorials/sobel-laplacian-edge-detectors/)
- [Keras Blog: How convolutional neural networks see the world](https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html)
- [The 9 Deep Learning Papers You Need To Know About](https://adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html)
- [Python based visualization repo for CNNs](https://github.com/HarisIqbal88/PlotNeuralNet)
- [Computing Receptive Fields of Convolutional Neural Networks](https://distill.pub/2019/computing-receptive-fields/)
- [Deconvolution and Checkerboard Artifacts](https://distill.pub/2016/deconv-checkerboard/)
- [Understanding Convolution in Deep Learning](http://timdettmers.com/2015/03/26/convolution-deep-learning/)    
- [What do we learn from region based object detectors (Faster R-CNN, R-FCN, FPN)]
- [How Convolutional Neural Networks see the World](https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html)
- [Attention in Neural Networks and How to Use It](http://akosiorek.github.io/ml/2017/10/14/visual-attention.html)
- [Neural Networks - A Systematic Introduction (FU Berlin)](https://page.mi.fu-berlin.de/rojas/neural/neuron.pdf)
- [Deep Learning - The Straight Dope (contains notebooks designed to teach deep learning)](https://gluon.mxnet.io/)
- [Computing Receptive Fields of Convolutional Neural Networks](https://distill.pub/2019/computing-receptive-fields/)
- Stanford: Convolutional Neural Networks for Visual Recognition
	- [Videos](http://cs231n.stanford.edu/)
	- [Assignements und notes](http://cs231n.github.io/) 
 

### Autoencoders

- [PCA](http://www.cs.cmu.edu/~guestrin/Class/15781/slides/pca-mdps-annotated.pdf)
- [Introducing Variational Autoencoders (in Prose and Code)](https://blog.fastforwardlabs.com/2016/08/12/introducing-variational-autoencoders-in-prose-and.html)

### Variational Autoencoders

- [A Tutorial on Variational Autoencoders](https://arxiv.org/pdf/1606.05908.pdf)



### Reinforcement Learning

- [Statistical Reinforcement Learning (Lecture)](http://nanjiang.cs.illinois.edu/cs598/)
- [Practical Reinforcement Learning (Course)](https://github.com/yandexdataschool/Practical_RL)



## LSTMs:

- [Understanding LSTM and its diagrams](https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714)
- [The most comprehensive yet simple and fun RNN/LSTM tutorial on the Internet.](https://ayearofai.com/rohan-lenny-3-recurrent-neural-networks-10300100899b)


### Hyperparameter Optimization / Neural Architecture Search / etc:

- [Using Machine Learning to Explore Neural Network Architecture](https://ai.googleblog.com/2017/05/using-machine-learning-to-explore.html)


### Software / Languages / etch

- [R vs Python: Image Classification with Keras](https://towardsdatascience.com/r-vs-python-image-classification-with-keras-1fa99a8fef9b)
- H20 related stuff: 
	- [Repository that contains the H2O presentation for Trevor Hastie and Rob Tibshirani's Statistical Learning and Data Mining IV course in Washington, DC on October 19, 2016.](https://github.com/ledell/sldm4-h2o)
	- [Deep Learning with H2O - PDF/Book](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/booklets/DeepLearningBooklet.pdf)


### Nice Demos and Vizualisations

- [Deep Traffic](https://selfdrivingcars.mit.edu/deeptraffic/)


### Material for Exercises

- [Neural networks Exercises (Part-1)](https://www.r-bloggers.com/neural-networks-exercises-part-1/)


- M. Deisenroth, A. Faisal, C. Ong. Mathematics for Machine Learning. [URL](https://mml-book.github.io/book/mml-book.pdf)
- L. Wassermann. All of Statistics. [URL](http://egrcc.github.io/docs/math/all-of-statistics.pdf)
- H. Wickham, G. Grolemund. R for Data Science. [URL](https://r4ds.had.co.nz/)
- Introductory R course on datacamp.com [URL](https://learn.datacamp.com/courses/free-introduction-to-r)


## Machine Learning

- K. Kersting, C. Lampert, C. Rothkopf. Wie Maschinen Lernen. Springer, 2019. [URL](https://link.springer.com/book/10.1007/978-3-658-26763-6) *German, informal, intuitive introduction to ML. Lower than BSc level, maybe more targeted at pupils or a non-academic audience. Read if you want a very light-weight introduction into the field, or buy as present for relatives and friends if they ask what you are doing.*
- G. James, D. Witten, T. Hastie, R. Tibshirani. An Introduction to Statistical Learning. MIT Press, 2010. [URL](http://www-bcf.usc.edu/~gareth/ISL/) *Beginner-level introduction with applications in R. Very well suited for the BSc level.*
- T. Hastie, R. Tibshirani, J. Friedman. The Elements of Statistical Learning. Springer, 2009. [URL](https://web.stanford.edu/~hastie/ElemStatLearn/) *Standard reference for statistics-flavored ML.*
- C. M. Bishop. Pattern Recognition and Machine Learning. Springer, 2006. [URL](http://research.microsoft.com/en-us/um/people/cmbishop/prml/) *Standard reference for ML-flavored ML.*
- S. Shalev-Shwartz, S. Ben-David. Understanding machine learning: From Theory to Algorithms. Cambridge University Press, 2014. [URL](https://www.cs.huji.ac.il/w~shais/UnderstandingMachineLearning/) *Great, thorough introduction to ML theory. Math-style book with definitions and proofs.*
- E. Alpaydin. Introduction to Machine Learning. MIT Press, 2010. [URL](http://www.cmpe.boun.edu.tr/~ethem/i2ml2e/) *Standard reference with broad coverage; easy to read.*
- K. Murphy. Machine Learning: a Probabilistic Perspective. MIT Press, 2012. [URL](https://probml.github.io/pml-book/book0.html) *Standard reference; quite extensive; statistical/probabilistic lens.*
- F. Provost, T. Fawcett. Data Science for Business. Oâ€™Reilly, 2013. [URL](https://book.akij.net/eBooks/2018/May/5aef50939a868/Data_Science_for_Bus.pdf) *A very good, applied and easy-to-read book by 2 well-known ML scientists. Contains many practical aspects that are missing in other references. Probably a good idea to read this in any case.*
- N. Japkowicz. Evaluating Learning Algorithms (A Classification Perspective). Cambridge University Press, 2011. *Nice reading on performance measures, resampling methods and (some) statistical tests for benchmarking in ML; only for classification.*
- B. Bischl et al. Hyperparameter Optimization: Foundations, Algorithms, Best Practices and Open Challenges. arXiv preprint 2021. [URL](https://arxiv.org/pdf/2107.05847.pdf) *Our tutorial paper on HPO.*
- I. Goodfellow, Y. Bengio, A. Courville. Deep Learning. MIT Press, 2016. [URL](https://www.deeplearningbook.org/) *Standard, modern reference for DL.*

## Mathematical & Statistical Theory

- G. Strang. Linear Algebra and Learning from Data. Cambridge University Press, 2019. *Serious course on matrices and applied linear algebra.*
- S. Axler. Linear Algebra Done Right. Springer, 2015. [URL](https://link.springer.com/content/pdf/10.1007%2F978-3-319-11080-6.pdf) *Linear Algebra from a more theoretical but still beginner-friendly perspective*
- A. M. Mood, F. A. Graybill, D. C. Boes. Introduction to the Theory of Statistics, McGraw-Hill 1974. [URL](https://www.fulviofrisone.com/attachments/article/446/Introduction%20to%20the%20theory%20of%20statistics%20by%20MOOD.pdf) *Beginner-friendly intro to statistics; bit on the mathy side.*
- J. Watt, R. Borhani, A. Katsaggelos. Machine Learning Refined. Cambridge University Press, 2020. [URL](https://github.com/jermwatt/machine_learning_refined) *Check chapters 2-4 plus Appendix for insightful explanations and visualizations of a variety of optimization concepts.*
- T. M. Cover, J. A. Thomas. Elements of Information Theory. Wiley, 2006. [URL](http://staff.ustc.edu.cn/~cgong821/Wiley.Interscience.Elements.of.Information.Theory.Jul.2006.eBook-DDU.pdf) *Good intro to information theory in first hundred pages, though lacking cross-connections to ML / statistics.*

## Python Programming

- J. VanderPlas. Python Data Science Handbook: Essential Tools for working with Data. 2016. 
Or use the online website such as:
- [URL](https://pythonprogramming.net/)
- [URL](https://www.pythontutorial.net/)


## R Programming

- N. Matloff. The Art of R Programming. No Starch Press, 2011. [URL](https://diytranscriptomics.com/Reading/files/The%20Art%20of%20R%20Programming.pdf)

We use the **mlr3** package for machine learning in R quite heavily.
- Central project page and learning resources: https://mlr3.mlr-org.com/, in particular
  - the [book](https://mlr3book.mlr-org.com/),
  - the [gallery](https://mlr3gallery.mlr-org.com/), and   
  - the [cheatsheets](https://cheatsheets.mlr-org.com/).
- GitHub page: https://github.com/mlr-org/mlr3

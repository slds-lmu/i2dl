<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Introduction to Deep Learning (I2DL) on Introduction to Machine Learning (I2ML)</title><link>https://slds-lmu.github.io/i2dl/</link><description>Recent content in Introduction to Deep Learning (I2DL) on Introduction to Machine Learning (I2ML)</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://slds-lmu.github.io/i2dl/index.xml" rel="self" type="application/rss+xml"/><item><title>Chapter 01.01: Introduction</title><link>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-01-intro/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-01-intro/</guid><description>&lt;p>In this section, we introduces the relationship of DL and ML, give basic intro about feature learning, and discuss the use-cases and data types for DL methods.&lt;/p></description></item><item><title>Chapter 01.02: Single Neuron</title><link>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-02-single-neuron/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-02-single-neuron/</guid><description>&lt;p>In this section we explain the graphical representation of a single neuron and describe affine transformations and non-linear activation functions. Moreover, we talk about the hypothesis spaces of a single neuron and name some typical loss functions.&lt;/p></description></item><item><title>Chapter 01.03: XOR Problem</title><link>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-03-xor/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-03-xor/</guid><description>&lt;p>Example problem a single neuron can not solve but a single hidden layer net can!&lt;/p></description></item><item><title>Chapter 01.04: Single Hidden Layer NN</title><link>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-04-singlenn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-04-singlenn/</guid><description>&lt;p>We introduce architecture of single hidden layer neural networks and discuss the advantage of hidden layers. Then, we explain the typical (non-linear) activation
functions.&lt;/p></description></item><item><title>Chapter 01.05: Single Hidden Layer Networks for Multi-Class Classification</title><link>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-05-nn4multiclass/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-05-nn4multiclass/</guid><description>&lt;p>In this section, we discuss a neural network architectures for multi-class classification, softmax activation function as well as the Softmax loss.&lt;/p></description></item><item><title>Chapter 01.06: MLP: Multi-Layer Feedforward Neural Networks</title><link>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-06-multilayernn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-06-multilayernn/</guid><description>&lt;p>Architectures of deep neural networks and deep neural networks as chained functions are the learning goal of this part.&lt;/p></description></item><item><title>Chapter 01.07: Optimization</title><link>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-07-matrixnotation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-07-matrixnotation/</guid><description>&lt;p>In this section we learn about compact representation of neural network, vector notation for neuron layers, vector and matrix notation of bias and weight parameters.&lt;/p></description></item><item><title>Chapter 01.08: Universal Approximation</title><link>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-08-universalapproximation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-08-universalapproximation/</guid><description>&lt;p>Universal approximation theorem for one-hidden-layer neural networks and pros and cons of a low approximation error are the learning goal of this section.&lt;/p></description></item><item><title>Chapter 01.09: Brief History</title><link>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-09-history/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-09-history/</guid><description>&lt;p>We overview history of DL development.&lt;/p></description></item><item><title>Chapter 02.01: Loss Functions for Regression</title><link>https://slds-lmu.github.io/i2dl/chapters/02_optimization1/02-01-basic-training/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/02_optimization1/02-01-basic-training/</guid><description>&lt;p>Empirical risk minimization, gradient descent, and stochastic gradient descent are three subtopic which we explain in this chapter.&lt;/p></description></item><item><title>Chapter 02.02: Chain Rule and Computational Graphs</title><link>https://slds-lmu.github.io/i2dl/chapters/02_optimization1/02-02-computational-graph/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/02_optimization1/02-02-computational-graph/</guid><description>&lt;p>In this section, we explain the chain rule of calculus, and it&amp;rsquo;s computational graphs.&lt;/p></description></item><item><title>Chapter 02.03: Basic Backpropagation 1</title><link>https://slds-lmu.github.io/i2dl/chapters/02_optimization1/02-03-backprob1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/02_optimization1/02-03-backprob1/</guid><description>&lt;p>This section introduces forward and backward passes, chain rule, and the details of backprop in deep learning.&lt;/p></description></item><item><title>Chapter 02.04: Basic Backpropagation 2</title><link>https://slds-lmu.github.io/i2dl/chapters/02_optimization1/02-04-backprob2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/02_optimization1/02-04-backprob2/</guid><description>&lt;p>We continue our discussion about backpropagation in formalism and recursion.&lt;/p></description></item><item><title>Chapter 02.05: Hardware and Software</title><link>https://slds-lmu.github.io/i2dl/chapters/02_optimization1/02-05-hardware/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/02_optimization1/02-05-hardware/</guid><description>&lt;p>This section introduces GPU training for accelerated learning of neural networks, software for hardware support, and deep learning software platforms.&lt;/p></description></item><item><title>Chapter 03.01: Basic Regularization</title><link>https://slds-lmu.github.io/i2dl/chapters/03_regularization/03-01-basic-reg/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/03_regularization/03-01-basic-reg/</guid><description>&lt;p>In this section we discuss regularized cost functions, norm penalties, weight decay, and equivalence with constrained optimization.&lt;/p></description></item><item><title>Chapter 03.02: Regularization in Non-Linear Models and Bayesian Priors</title><link>https://slds-lmu.github.io/i2dl/chapters/03_regularization/03-02-reg-detail/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/03_regularization/03-02-reg-detail/</guid><description>&lt;p>In this section, we motivate regularization from a Bayesian perspective.&lt;/p></description></item><item><title>Chapter 03.03: Geometric Analysis of L2 Regularization and Weight Decay</title><link>https://slds-lmu.github.io/i2dl/chapters/03_regularization/03-03-geometric/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/03_regularization/03-03-geometric/</guid><description>&lt;p>In this section, we provide a geometric understanding of \(L2\) regularization, showing how parameters are shrunk according to the eigenvalues of the Hessian of empirical risk, and discuss its correspondence to weight decay.&lt;/p></description></item><item><title>Chapter 03.04: Early Stopping</title><link>https://slds-lmu.github.io/i2dl/chapters/03_regularization/03-04-early-stop/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/03_regularization/03-04-early-stop/</guid><description>&lt;p>In this section, we introduce early stopping and show how it can act as a regularizer.&lt;/p></description></item><item><title>Chapter 03.05: Regularization</title><link>https://slds-lmu.github.io/i2dl/chapters/03_regularization/03-05-dropout/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/03_regularization/03-05-dropout/</guid><description>&lt;p>In this section, we explain ensemble method, dropout and data augmentation.&lt;/p></description></item><item><title>Chapter 04.01: Challenges in Optimization</title><link>https://slds-lmu.github.io/i2dl/chapters/04_optimization2/04-01-challenges_opt/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/04_optimization2/04-01-challenges_opt/</guid><description>&lt;p>In this section, we summarize several of the most prominent challenges regarding training of deep neural networks such as Ill-Conditioning Local Minima, Saddle Points, Cliffs and Exploding Gradients.&lt;/p></description></item><item><title>Chapter 04.02: Measures Regression</title><link>https://slds-lmu.github.io/i2dl/chapters/04_optimization2/04-02-adv-opt/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/04_optimization2/04-02-adv-opt/</guid><description>&lt;p>In this section we introduce several advanced techniques for optimization of neural network such as learning rate schedules, adaptive learning rates, and batch normalization.&lt;/p></description></item><item><title>Chapter 04.03: Modern Activation Functions</title><link>https://slds-lmu.github.io/i2dl/chapters/04_optimization2/04-03-activation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/04_optimization2/04-03-activation/</guid><description>&lt;p>We explain challenges in optimization related to action function and introduce activation for hidden and output units.&lt;/p></description></item><item><title>Chapter 04.04: Network Initialization</title><link>https://slds-lmu.github.io/i2dl/chapters/04_optimization2/04-04-init/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/04_optimization2/04-04-init/</guid><description>&lt;p>This part describe why initialization is important and explain weight and bias initialization.&lt;/p></description></item><item><title>Chapter 05.01: Introduction of Convolution Neural Networks (CNNs)</title><link>https://slds-lmu.github.io/i2dl/chapters/05_cnn/05-01-intro-cnn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/05_cnn/05-01-intro-cnn/</guid><description>&lt;p>In this part, we introduce the CNNs and when we can apply CNNs instead of FCN.&lt;/p></description></item><item><title>Chapter 05.02: Convolutional Operation</title><link>https://slds-lmu.github.io/i2dl/chapters/05_cnn/05-02-conv2d/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/05_cnn/05-02-conv2d/</guid><description>&lt;p>We learn about filters and convolution operation in this section.&lt;/p></description></item><item><title>Chapter 05.03: Properties of Convolution</title><link>https://slds-lmu.github.io/i2dl/chapters/05_cnn/05-03-cnn-properties/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/05_cnn/05-03-cnn-properties/</guid><description>&lt;p>We introduce three important properties by CNNs.&lt;/p></description></item><item><title>Chapter 05.04: CNN Components</title><link>https://slds-lmu.github.io/i2dl/chapters/05_cnn/05-04-cnn-component/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/05_cnn/05-04-cnn-component/</guid><description>&lt;p>We explain input channel, padding, stride, and pooling as CNN component.&lt;/p></description></item><item><title>Chapter 05.05: CNN Application</title><link>https://slds-lmu.github.io/i2dl/chapters/05_cnn/05-05-cnn-app/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/05_cnn/05-05-cnn-app/</guid><description>&lt;p>We overview some successful application of CNN in visual recognition.&lt;/p></description></item><item><title>Chapter 05.06: Convolutions- Mathematical Perspective</title><link>https://slds-lmu.github.io/i2dl/chapters/05_cnn/05-06-cnn-math/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/05_cnn/05-06-cnn-math/</guid><description>&lt;p>We explain the differences between convolution operation and cross-correlation.&lt;/p></description></item><item><title>Chapter 06.01: 1D/ 2D/ 3D Convolutions</title><link>https://slds-lmu.github.io/i2dl/chapters/06_moderncnn/06-01-con1d/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/06_moderncnn/06-01-con1d/</guid><description>&lt;p>We describe convolution operation for different types of data.&lt;/p></description></item><item><title>Chapter 06.02: Important Types of Convolutions</title><link>https://slds-lmu.github.io/i2dl/chapters/06_moderncnn/06-02-dilated/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/06_moderncnn/06-02-dilated/</guid><description>&lt;p>Dilated Convolutions and Transposed Convolutions are two important types of CNNs that we introduce them in this section and describe the advantage and problem which they addressed.&lt;/p></description></item><item><title>Chapter 06.03: Separable Convolutions and Flattening</title><link>https://slds-lmu.github.io/i2dl/chapters/06_moderncnn/06-03-separable/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/06_moderncnn/06-03-separable/</guid><description>&lt;p>In this section, we explain separable CNNs and Flattening.&lt;/p></description></item><item><title>Chapter 06.04: Modern Convolutional Architecture - Part1</title><link>https://slds-lmu.github.io/i2dl/chapters/06_moderncnn/06-04-arch1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/06_moderncnn/06-04-arch1/</guid><description>&lt;p>In this section, we explain the recent popular CNN-based architecture.&lt;/p></description></item><item><title>Chapter 06.05: Modern Convolutional Architecture - Part2</title><link>https://slds-lmu.github.io/i2dl/chapters/06_moderncnn/06-05-arch2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/06_moderncnn/06-05-arch2/</guid><description>&lt;p>In this section, we explain the recent popular CNN-based architecture.&lt;/p></description></item><item><title>Chapter 06.06: Adversarial Robustness and Examples</title><link>https://slds-lmu.github.io/i2dl/chapters/06_moderncnn/06-06-adv/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/06_moderncnn/06-06-adv/</guid><description>&lt;p>Adversarial machine learning studies techniques which attempt to fool machine learning models through malicious input, we explain adversarial examples and adversarial training in this section.&lt;/p></description></item><item><title>Chapter 07.01: Recurrent Neural Networks - Introduction</title><link>https://slds-lmu.github.io/i2dl/chapters/07_rnn/07-01-intro-rnn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/07_rnn/07-01-intro-rnn/</guid><description>&lt;p>Recurrent neural networks (RNN) are the state of the art algorithm for sequential data and are used by Apple&amp;rsquo;s Siri and Google&amp;rsquo;s voice search. It is the first algorithm that remembers its input, due to an internal memory, which makes it perfectly suited for machine learning problems that involve sequential data.&lt;/p></description></item><item><title>Chapter 07.02: Recurrent Neural Networks - Backpropogation</title><link>https://slds-lmu.github.io/i2dl/chapters/07_rnn/07-02-backprob-rnn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/07_rnn/07-02-backprob-rnn/</guid><description>&lt;p>In this section we explain the backpropagation for RNN as well as exploiding and vanishing gradients.&lt;/p></description></item><item><title>Chapter 07.03: Modern Recurrent Neural Networks</title><link>https://slds-lmu.github.io/i2dl/chapters/07_rnn/07-03-modern-rnn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/07_rnn/07-03-modern-rnn/</guid><description>&lt;p>We explain how modern RNN such as LSTM, GRU, and Bidirectional RNN addressed problem of exploding and vanishing gradient by conventional RNN.&lt;/p></description></item><item><title>Chapter 08.01: Applications of RNNs</title><link>https://slds-lmu.github.io/i2dl/chapters/08_modernrnn/08-01-rnn-app/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/08_modernrnn/08-01-rnn-app/</guid><description>&lt;p>In this section, we overview some applications of RNN in NLP and computer vision.&lt;/p></description></item><item><title>Chapter 08.02: Problem Definition</title><link>https://slds-lmu.github.io/i2dl/chapters/08_modernrnn/08-02-att/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/08_modernrnn/08-02-att/</guid><description/></item><item><title>Chapter 09.01: Unsupervised Learning</title><link>https://slds-lmu.github.io/i2dl/chapters/09_ae/09-01-unsupervised/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/09_ae/09-01-unsupervised/</guid><description>&lt;p>We explain unsupervised learning tasks and unsupervised representation learning.&lt;/p></description></item><item><title>Chapter 09.02: Manifold Learning</title><link>https://slds-lmu.github.io/i2dl/chapters/09_ae/09-02-manifold/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/09_ae/09-02-manifold/</guid><description>&lt;p>In this section, we explain the manifold hypothesis and manifold learning with AEs.&lt;/p></description></item><item><title>Chapter 09.03: Auto-Encoders</title><link>https://slds-lmu.github.io/i2dl/chapters/09_ae/09-03-ae/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/09_ae/09-03-ae/</guid><description>&lt;p>In this section, we will explain task and structure of Auto-Encoder.&lt;/p></description></item><item><title>Chapter 09.03: Regularized Autoencoders</title><link>https://slds-lmu.github.io/i2dl/chapters/09_ae/09-04-reg-ae/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/09_ae/09-04-reg-ae/</guid><description>&lt;p>In this section, we explain overcomplete AEs, sparse AEs, denoising AEs and contractive AEs.&lt;/p></description></item><item><title>Chapter 09.05: Specific Autoencoders and Applications</title><link>https://slds-lmu.github.io/i2dl/chapters/09_ae/09-05-specific-ae/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/09_ae/09-05-specific-ae/</guid><description>&lt;p>For the image domain, using convolutions is advantageous. Can we also make use of them in AEs? In this section, we answer this question and overview some applications.&lt;/p></description></item><item><title>Chapter 10.01: Introduction to Generative Models</title><link>https://slds-lmu.github.io/i2dl/chapters/10_gan/10-01-generative-model/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/10_gan/10-01-generative-model/</guid><description>&lt;p>In this section, we introduce the generative model, a powerful family of machine learning.&lt;/p></description></item><item><title>Chapter 10.02: Probabilistic graphical models</title><link>https://slds-lmu.github.io/i2dl/chapters/10_gan/10-02-prob-graph/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/10_gan/10-02-prob-graph/</guid><description>&lt;p>In this section, we explain probabilistic graphical models, latent variables, and directed graphical models.&lt;/p></description></item><item><title>Chapter 10.03: Tuning with Variation Autoencoders</title><link>https://slds-lmu.github.io/i2dl/chapters/10_gan/10-03-vae/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/10_gan/10-03-vae/</guid><description>&lt;p>Instead of mapping the input into a fixed vector, we want to map it into a
distribution.&lt;/p></description></item><item><title>Chapter 10.04: Generative Adversarial Networks (GANs)</title><link>https://slds-lmu.github.io/i2dl/chapters/10_gan/10-04-gan-intro/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/10_gan/10-04-gan-intro/</guid><description>&lt;p>Generative adversarial networks (GANs) are an exciting recent innovation in machine learning. GANs are generative models: they create new data instances that resemble your training data. For example, GANs can create images that look like photographs of human faces, even though the faces don&amp;rsquo;t belong to any real person.&lt;/p></description></item><item><title>Chapter 10.05: GAN variants</title><link>https://slds-lmu.github.io/i2dl/chapters/10_gan/10-05-gan-var/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/10_gan/10-05-gan-var/</guid><description>&lt;p>We explain non-saturating loss and conditional GANs in this part.&lt;/p></description></item><item><title>Chapter 10.06: Challenges for GAN Optimization</title><link>https://slds-lmu.github.io/i2dl/chapters/10_gan/10-06-gan-challenge/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/10_gan/10-06-gan-challenge/</guid><description>&lt;p>We explain challenges of GANs model such as no convergence to fix point
as well as problems of adversarial setting.&lt;/p></description></item><item><title>Cheat Sheets</title><link>https://slds-lmu.github.io/i2dl/appendix/01_cheat_sheets/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/appendix/01_cheat_sheets/</guid><description>I2ML :: Basics Download &amp;raquo;cheatsheet_notation.pdf&amp;laquo; I2ML :: Evaluation &amp;amp; Tuning Download &amp;raquo;cheatsheet_eval_tuning.pdf&amp;laquo; TensorFlow &amp;amp; PyTorch :: Introduction Download &amp;raquo;cheatsheet_01.pdf&amp;laquo; TensorFlow &amp;amp; PyTorch :: Data Manipulation, Linear Algebra, Automatic Differentiation, Probability Download &amp;raquo;cheatsheet_02.pdf&amp;laquo; TensorFlow &amp;amp; PyTorch :: Linear Regression Download &amp;raquo;cheatsheet_03.pdf&amp;laquo; TensorFlow &amp;amp; PyTorch :: MLP &amp;amp; Activation Functions Download &amp;raquo;cheatsheet_04.pdf&amp;laquo; TensorFlow &amp;amp; PyTorch :: FCN &amp;amp; CNN Download &amp;raquo;cheatsheet_06.pdf&amp;laquo; TensorFlow &amp;amp; PyTorch :: Batch Normalization Download &amp;raquo;cheatsheet_6-2.pdf&amp;laquo; TensorFlow &amp;amp; PyTorch :: Modern CNN Download &amp;raquo;cheatsheet_6-3.</description></item><item><title>Errata</title><link>https://slds-lmu.github.io/i2dl/appendix/02_errata/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/appendix/02_errata/</guid><description>Errata in the slides shown in the videos</description></item></channel></rss>
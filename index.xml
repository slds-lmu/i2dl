<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Introduction to Deep Learning (I2DL) on Introduction to Machine Learning (I2ML)</title><link>https://slds-lmu.github.io/i2dl/</link><description>Recent content in Introduction to Deep Learning (I2DL) on Introduction to Machine Learning (I2ML)</description><generator>Hugo</generator><language>en-us</language><atom:link href="https://slds-lmu.github.io/i2dl/index.xml" rel="self" type="application/rss+xml"/><item><title>Chapter 01.01: Introduction</title><link>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-01-intro/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-01-intro/</guid><description>&lt;p>In this section, we introduce the relationship of DL and ML, give a basic intro about feature learning, and discuss the use-cases and data types for DL methods.&lt;/p></description></item><item><title>Chapter 01.02: Single Neuron</title><link>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-02-single-neuron/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-02-single-neuron/</guid><description>&lt;p>In this section we explain the graphical representation of a single neuron and describe affine transformations and non-linear activation functions. Moreover, we talk about the hypothesis space of a single neuron and name some typical loss functions.&lt;/p></description></item><item><title>Chapter 01.03: XOR Problem</title><link>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-03-xor/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-03-xor/</guid><description>&lt;p>In this subsection, we present the XOR problem, a famous example a single neuron can not solve but a single hidden layer net can solve.&lt;/p></description></item><item><title>Chapter 01.04: Single Hidden Layer NN</title><link>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-04-singlenn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-04-singlenn/</guid><description>&lt;p>In this subchapter, we introduce the architecture of single hidden layer neural networks and discuss the advantages of hidden layers. In addition, we present some typical (non-linear) activation functions.&lt;/p></description></item><item><title>Chapter 01.05: Single Hidden Layer Networks for Multi-Class Classification</title><link>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-05-nn4multiclass/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-05-nn4multiclass/</guid><description>&lt;p>In this subsection, we discuss neural network architectures for multi-class classification, the softmax activation function as well as the softmax loss.&lt;/p></description></item><item><title>Chapter 01.06: MLP: Multi-Layer Feedforward Neural Networks</title><link>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-06-multilayernn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-06-multilayernn/</guid><description>&lt;p>In this subchapter, we present the architecture of deep neural networks and introduce deep neural networks as chained functions.&lt;/p></description></item><item><title>Chapter 01.07: Matrix Notation</title><link>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-07-matrixnotation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-07-matrixnotation/</guid><description>&lt;p>In this section we explain the compact representation of neural networks, the vector notation for neuron layers and both the vector and matrix notation for bias and weight parameters.&lt;/p></description></item><item><title>Chapter 01.08: Universal Approximation</title><link>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-08-universalapproximation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-08-universalapproximation/</guid><description>&lt;p>Here we present the universal approximation theorem for one-hidden-layer neural networks. In addition, we discuss the pros and cons of a low approximation error.&lt;/p></description></item><item><title>Chapter 01.09: Brief History</title><link>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-09-history/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/01_intro/01-09-history/</guid><description>&lt;p>In this subsection we present an overview of the history of DL development.&lt;/p></description></item><item><title>Chapter 02.01: Basic Training</title><link>https://slds-lmu.github.io/i2dl/chapters/02_optimization1/02-01-basic-training/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/02_optimization1/02-01-basic-training/</guid><description>&lt;p>This subchapter covers essential principles of neural network training, starting with empirical risk minimization (ERM) and gradient descent (GD). Additionally, it introduces stochastic gradient descent (SGD) as a computationally efficient alternative to GD.&lt;/p></description></item><item><title>Chapter 02.02: Chain Rule and Computational Graphs</title><link>https://slds-lmu.github.io/i2dl/chapters/02_optimization1/02-02-computational-graph/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/02_optimization1/02-02-computational-graph/</guid><description>&lt;p>In this subsection, we explain the chain rule of calculus, and the corresponding computational graphs.&lt;/p></description></item><item><title>Chapter 02.03: Basic Backpropagation I</title><link>https://slds-lmu.github.io/i2dl/chapters/02_optimization1/02-03-backprob1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/02_optimization1/02-03-backprob1/</guid><description>&lt;p>This subsection introduces forward and backward passes, the chain rule, and the details of backpropagation in deep learning.&lt;/p></description></item><item><title>Chapter 02.04: Basic Backpropagation II</title><link>https://slds-lmu.github.io/i2dl/chapters/02_optimization1/02-04-backprob2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/02_optimization1/02-04-backprob2/</guid><description>&lt;p>In this subsection we focus on the formalism of backpropagation and the concept of recursion in this context.&lt;/p></description></item><item><title>Chapter 02.05: Hardware and Software</title><link>https://slds-lmu.github.io/i2dl/chapters/02_optimization1/02-05-hardware/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/02_optimization1/02-05-hardware/</guid><description>&lt;p>This subsection introduces GPU training for accelerated learning of neural networks, software for hardware support, and deep learning software platforms.&lt;/p></description></item><item><title>Chapter 03.01: Basic Regularization</title><link>https://slds-lmu.github.io/i2dl/chapters/03_regularization/03-01-basic-reg/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/03_regularization/03-01-basic-reg/</guid><description>&lt;p>In this section we discuss regularized cost functions, norm penalties, weight decay, and equivalence with constrained optimization.&lt;/p></description></item><item><title>Chapter 03.02: Ridge Regularization</title><link>https://slds-lmu.github.io/i2dl/chapters/03_regularization/03-02-reg-ridge/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/03_regularization/03-02-reg-ridge/</guid><description>&lt;p>In this section, we introduce Ridge regression as a key approach to regularizing linear models (Material provided by: &lt;a href="https://slds-lmu.github.io/i2ml/chapters/15_regularization/">I2ML/SL lecture&lt;/a>.
)&lt;/p></description></item><item><title>Chapter 03.03: Regularization in Non-Linear Models</title><link>https://slds-lmu.github.io/i2dl/chapters/03_regularization/03-03-nonlin/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/03_regularization/03-03-nonlin/</guid><description>&lt;p>In this section, we introduce regularization in non-linear models like neural networks.
(Material provided by: &lt;a href="https://slds-lmu.github.io/i2ml/chapters/15_regularization/">I2ML/SL lecture&lt;/a>.
)&lt;/p></description></item><item><title>Chapter 03.04: Geometric Analysis of L2 Regularization and Weight Decay</title><link>https://slds-lmu.github.io/i2dl/chapters/03_regularization/03-04-geometric/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/03_regularization/03-04-geometric/</guid><description>&lt;p>In this section, we provide a geometric understanding of \(L2\) regularization, showing how parameters are shrunk according to the eigenvalues of the Hessian of empirical risk, and discuss its correspondence to weight decay.
(Material provided by: &lt;a href="https://slds-lmu.github.io/i2ml/chapters/15_regularization/">I2ML/SL lecture&lt;/a>.
)&lt;/p></description></item><item><title>Chapter 03.05: Early Stopping</title><link>https://slds-lmu.github.io/i2dl/chapters/03_regularization/03-05-early-stop/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/03_regularization/03-05-early-stop/</guid><description>&lt;p>In this section, we introduce early stopping and show how it can act as a regularizer.&lt;/p></description></item><item><title>Chapter 03.06: Regularization</title><link>https://slds-lmu.github.io/i2dl/chapters/03_regularization/03-06-dropout/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/03_regularization/03-06-dropout/</guid><description>&lt;p>In this section, we explain ensemble methods, dropout and data augmentation.&lt;/p></description></item><item><title>Chapter 04.01: Challenges in Optimization</title><link>https://slds-lmu.github.io/i2dl/chapters/04_optimization2/04-01-challenges_opt/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/04_optimization2/04-01-challenges_opt/</guid><description>&lt;p>In this subsection, we summarize several of the most prominent challenges regarding training of deep neural networks such as ill-conditioning, local minima, saddle points, cliffs and exploding gradients.&lt;/p></description></item><item><title>Chapter 04.02: Measures Regression</title><link>https://slds-lmu.github.io/i2dl/chapters/04_optimization2/04-02-adv-opt/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/04_optimization2/04-02-adv-opt/</guid><description>&lt;p>In this section we introduce several advanced techniques for optimization of neural network such as learning rate schedules, adaptive learning rates, and batch normalization.&lt;/p></description></item><item><title>Chapter 04.03: Modern Activation Functions</title><link>https://slds-lmu.github.io/i2dl/chapters/04_optimization2/04-03-activation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/04_optimization2/04-03-activation/</guid><description>&lt;p>In this subchapter, we explain challenges in optimization related to activation functions. In addition, we introduce activations for both hidden and output units.&lt;/p></description></item><item><title>Chapter 04.04: Network Initialization</title><link>https://slds-lmu.github.io/i2dl/chapters/04_optimization2/04-04-init/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/04_optimization2/04-04-init/</guid><description>&lt;p>In this part we explain why initialization is crucial for neural network training. We introduce the concept of random weight initialization and explain initialization approaches for the biases of a NN.&lt;/p></description></item><item><title>Chapter 05.01: Introduction of Convolution Neural Networks (CNNs)</title><link>https://slds-lmu.github.io/i2dl/chapters/05_cnn/05-01-intro-cnn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/05_cnn/05-01-intro-cnn/</guid><description>&lt;p>This subchapter briefly covers the primary components of CNN architectures and explores applications of CNNs in fields like autonomous driving, medical imaging, and natural language processing.&lt;/p></description></item><item><title>Chapter 05.02: Convolutional Operation</title><link>https://slds-lmu.github.io/i2dl/chapters/05_cnn/05-02-conv2d/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/05_cnn/05-02-conv2d/</guid><description>&lt;p>This subchapter introduces convolutional operations, focusing on the role of filters in feature extraction. It explains how convolutional layers apply learned filters to images and also covers the 2D convolution operation in detail, illustrating the step-by-step computation process.&lt;/p></description></item><item><title>Chapter 05.03: Properties of Convolution</title><link>https://slds-lmu.github.io/i2dl/chapters/05_cnn/05-03-cnn-properties/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/05_cnn/05-03-cnn-properties/</guid><description>&lt;p>In this subchapter, we introduce sparse interactions, parameter sharing and the equivariance to translation, which are three important properties of convolution.&lt;/p></description></item><item><title>Chapter 05.04: CNN Components</title><link>https://slds-lmu.github.io/i2dl/chapters/05_cnn/05-04-cnn-component/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/05_cnn/05-04-cnn-component/</guid><description>&lt;p>This subchapter covers fundamental components of CNNs such as input channels, padding, stride, and pooling layers. Padding and stride settings control the dimensions and details retained in feature maps, while pooling layers i.a. reduce data size and improve computational efficiency. Together, these components enable CNNs to handle high-dimensional data effectively, capturing both local and global patterns in images.&lt;/p></description></item><item><title>Chapter 05.05: CNN Application</title><link>https://slds-lmu.github.io/i2dl/chapters/05_cnn/05-05-cnn-app/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/05_cnn/05-05-cnn-app/</guid><description>&lt;p>In this subchapter, we explore diverse applications of CNNs e.g. in visual recognition tasks, image classification, object detection, colorization, and semantic segmentation, illustrating their adaptability across domains.&lt;/p></description></item><item><title>Chapter 06.01: 1D/ 2D/ 3D Convolutions</title><link>https://slds-lmu.github.io/i2dl/chapters/06_moderncnn/06-01-con1d/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/06_moderncnn/06-01-con1d/</guid><description>&lt;p>In this subchapter we present the various types of convolutions in CNNs — 1D, 2D, and 3D — each suited for specific data structures and applications. One-dimensional convolutions process sequential data like time series, two-dimensional convolutions handle spatial data like images, and three-dimensional convolutions analyze volumetric data like videos or MRI scans.&lt;/p></description></item><item><title>Chapter 06.02: Dilated Convolutions and Transposed Convolutions</title><link>https://slds-lmu.github.io/i2dl/chapters/06_moderncnn/06-02-dilated/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/06_moderncnn/06-02-dilated/</guid><description>&lt;p>This subchapter explores dilated and transposed convolutions, specialized types of convolutional operations in CNNs. Dilated convolutions expand the receptive field without increasing the number of parameters, whereas transposed convolutions upsample data by increasing feature map dimensions, often used in applications requiring higher resolution outputs.&lt;/p></description></item><item><title>Chapter 06.03: Separable Convolutions and Flattening</title><link>https://slds-lmu.github.io/i2dl/chapters/06_moderncnn/06-03-separable/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/06_moderncnn/06-03-separable/</guid><description>&lt;p>This subsection introduces separable convolutions and the flattening process in CNNs. Separable convolutions improve computational efficiency, whilst flattening converts multidimensional feature maps into a 1D array i.a. bridging the gap between convolutional and dense layers.&lt;/p></description></item><item><title>Chapter 06.04: Modern Convolutional Architectures - Part I</title><link>https://slds-lmu.github.io/i2dl/chapters/06_moderncnn/06-04-arch1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/06_moderncnn/06-04-arch1/</guid><description>&lt;p>In this subsection, we explain the recent popular CNN-based architectures such as LeNet, AlexNet and VGG.&lt;/p></description></item><item><title>Chapter 06.05: Modern Convolutional Architectures - Part II</title><link>https://slds-lmu.github.io/i2dl/chapters/06_moderncnn/06-05-arch2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/06_moderncnn/06-05-arch2/</guid><description>&lt;p>In this subsection, we focus further on modern CNN architectures such as GoogleNet, ResNet and DenseNet.&lt;/p></description></item><item><title>Chapter 07.01: Introduction to RNNs</title><link>https://slds-lmu.github.io/i2dl/chapters/07_rnn/07-01-intro-rnn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/07_rnn/07-01-intro-rnn/</guid><description>&lt;p>Recurrent neural networks (RNN) are the state of the art algorithm for sequential data and are used by Apple&amp;rsquo;s Siri and Google&amp;rsquo;s voice search. It is the first algorithm that remembers its input, due to an internal memory, which makes it perfectly suited for machine learning problems that involve sequential data.&lt;/p></description></item><item><title>Chapter 07.02: Backpropogation in RNNs</title><link>https://slds-lmu.github.io/i2dl/chapters/07_rnn/07-02-backprob-rnn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/07_rnn/07-02-backprob-rnn/</guid><description>&lt;p>In this section we explain backpropagation for RNN as well as the phenomenon of exploding and vanishing gradients.&lt;/p></description></item><item><title>Chapter 07.03: Modern RNNs</title><link>https://slds-lmu.github.io/i2dl/chapters/07_rnn/07-03-modern-rnn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/07_rnn/07-03-modern-rnn/</guid><description>&lt;p>In this subchapter, we explain how modern RNN such as LSTM, GRU, and Bidirectional RNNs address the problem of exploding and vanishing gradients.&lt;/p></description></item><item><title>Chapter 07.04: Applications of RNNs</title><link>https://slds-lmu.github.io/i2dl/chapters/07_rnn/07-04-applics-rnn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/07_rnn/07-04-applics-rnn/</guid><description>&lt;p>This subsection focuses on common applications of RNNs, e.g. in the context of large language modelling or encoder-decoder architectures.&lt;/p></description></item><item><title>Chapter 07.05: Attention and Transformers</title><link>https://slds-lmu.github.io/i2dl/chapters/07_rnn/07-05-attention-transf-rnn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/07_rnn/07-05-attention-transf-rnn/</guid><description>&lt;p>In this subchapter, we introduce more recent sequence data modelling techniques such as attention and transformers.&lt;/p></description></item><item><title>Chapter 08.01: Unsupervised Learning</title><link>https://slds-lmu.github.io/i2dl/chapters/08_ae/08-01-unsupervised/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/08_ae/08-01-unsupervised/</guid><description>&lt;p>This subchapter provides an overview of unsupervised learning, focusing on discovering patterns and structures within unlabeled data. Key topics include clustering, dimensionality reduction, feature extraction, and generative modeling, each demonstrating different ways to learn and represent underlying data structures.&lt;/p></description></item><item><title>Chapter 08.02: Autoencoders</title><link>https://slds-lmu.github.io/i2dl/chapters/08_ae/08-02-ae/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/08_ae/08-02-ae/</guid><description>&lt;p>This subchapter covers the task and structure of AEs, which compress data into a lower-dimensional latent space and reconstruct it. In addition, we focus on undercomplete AEs, enforcing a &amp;ldquo;bottleneck&amp;rdquo; to focus on essential features. Linear undercomplete AEs with L2-reconstruction error approximate PCA by identifying principal components, while nonlinear AEs extend this capability to capture complex data patterns.&lt;/p></description></item><item><title>Chapter 08.03: Regularized Autoencoders</title><link>https://slds-lmu.github.io/i2dl/chapters/08_ae/08-03-reg-ae/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/08_ae/08-03-reg-ae/</guid><description>&lt;p>In this subsection, we explain overcomplete AEs, sparse AEs, denoising AEs and contractive AEs.&lt;/p></description></item><item><title>Chapter 08.04: Specific Autoencoders and Applications</title><link>https://slds-lmu.github.io/i2dl/chapters/08_ae/08-04-specific-ae/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/08_ae/08-04-specific-ae/</guid><description>&lt;p>This subchapter introduces convolutional autoencoders (ConvAEs), which utilize convolutional and transpose convolutional layers for processing image data. Furthermore, some practical applications e.g. the denoising of medical images or image compression are briefly presented.&lt;/p></description></item><item><title>Chapter 08.05: Manifold Learning</title><link>https://slds-lmu.github.io/i2dl/chapters/08_ae/08-05-manifold/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/08_ae/08-05-manifold/</guid><description>&lt;p>In this subchapter we explore the concept of manifold learning, focusing on the manifold hypothesis. In addition, we present AEs as tools for learning such manifolds.&lt;/p></description></item><item><title>Chapter 09.01: Generative Adversarial Networks (GANs)</title><link>https://slds-lmu.github.io/i2dl/chapters/09_gan/09-01-gan-intro/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/09_gan/09-01-gan-intro/</guid><description>&lt;p>This subchapter introduces GANs, which consist of a generator and a discriminator competing to create realistic data samples. In addition, key concepts discussed in this subchapter include the minimax loss function and challenges in training, such as stability issues and optimal discriminator requirements.&lt;/p></description></item><item><title>Chapter 09.02: GAN variants</title><link>https://slds-lmu.github.io/i2dl/chapters/09_gan/09-02-gan-var/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/09_gan/09-02-gan-var/</guid><description>&lt;p>This subsection introduces key GAN variants that address limitations in traditional GAN training. It covers non-saturating loss, which helps avoid vanishing gradients. Conditional GANs are also discussed, where additional data, such as labels or images, is used to guide the generation process, enabling more controlled and targeted outputs.&lt;/p></description></item><item><title>Chapter 09.03: Challenges for GAN Optimization</title><link>https://slds-lmu.github.io/i2dl/chapters/09_gan/09-03-gan-challenge/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/chapters/09_gan/09-03-gan-challenge/</guid><description>&lt;p>This subsection covers the main challenges in optimizing GANs, including non-convergence, mode collapse, and oscillating or chaotic behaviors during training.&lt;/p></description></item><item><title>Cheat Sheets</title><link>https://slds-lmu.github.io/i2dl/appendix/01_cheat_sheets/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/appendix/01_cheat_sheets/</guid><description>&lt;ul>
&lt;li>I2ML :: Basics&lt;/li>
&lt;/ul>


&lt;script src="https://cdn.jsdelivr.net/npm/pdfjs-dist@2.9.359/build/pdf.min.js" integrity="sha256-hEmjt7z3bB53X/awJyV81gmBLpVw2mj7EsvoJelZWow=" crossorigin="anonymous">&lt;/script>




 
 
 &lt;a href="https://github.com/slds-lmu/lecture_i2ml/raw/master/cheatsheets/cheatsheet_notation/cheatsheet_notation.pdf">
 &lt;button class="btn btn-primary" style="margin-bottom:3rem">
 Download &amp;raquo;cheatsheet_notation.pdf&amp;laquo;
 &lt;/button>
 &lt;/a>


&lt;ul>
&lt;li>I2ML :: Evaluation &amp;amp; Tuning&lt;/li>
&lt;/ul>


&lt;script src="https://cdn.jsdelivr.net/npm/pdfjs-dist@2.9.359/build/pdf.min.js" integrity="sha256-hEmjt7z3bB53X/awJyV81gmBLpVw2mj7EsvoJelZWow=" crossorigin="anonymous">&lt;/script>




 
 
 &lt;a href="https://github.com/slds-lmu/lecture_i2ml/raw/master/cheatsheets/cheatsheet_eval_tuning/cheatsheet_eval_tuning.pdf">
 &lt;button class="btn btn-primary" style="margin-bottom:3rem">
 Download &amp;raquo;cheatsheet_eval_tuning.pdf&amp;laquo;
 &lt;/button>
 &lt;/a>


&lt;ul>
&lt;li>TensorFlow &amp;amp; PyTorch :: Introduction&lt;/li>
&lt;/ul>


&lt;script src="https://cdn.jsdelivr.net/npm/pdfjs-dist@2.9.359/build/pdf.min.js" integrity="sha256-hEmjt7z3bB53X/awJyV81gmBLpVw2mj7EsvoJelZWow=" crossorigin="anonymous">&lt;/script>




 
 
 &lt;a href="https://github.com/slds-lmu/lecture_i2dl/raw/main/cheetsheets-pdf/cheatsheet_01.pdf">
 &lt;button class="btn btn-primary" style="margin-bottom:3rem">
 Download &amp;raquo;cheatsheet_01.pdf&amp;laquo;
 &lt;/button>
 &lt;/a>


&lt;ul>
&lt;li>TensorFlow &amp;amp; PyTorch :: Data Manipulation, Linear Algebra, Automatic Differentiation, Probability&lt;/li>
&lt;/ul>


&lt;script src="https://cdn.jsdelivr.net/npm/pdfjs-dist@2.9.359/build/pdf.min.js" integrity="sha256-hEmjt7z3bB53X/awJyV81gmBLpVw2mj7EsvoJelZWow=" crossorigin="anonymous">&lt;/script>




 
 
 &lt;a href="https://github.com/slds-lmu/lecture_i2dl/raw/main/cheetsheets-pdf/cheatsheet_02.pdf">
 &lt;button class="btn btn-primary" style="margin-bottom:3rem">
 Download &amp;raquo;cheatsheet_02.pdf&amp;laquo;
 &lt;/button>
 &lt;/a>


&lt;ul>
&lt;li>TensorFlow &amp;amp; PyTorch :: Linear Regression&lt;/li>
&lt;/ul>


&lt;script src="https://cdn.jsdelivr.net/npm/pdfjs-dist@2.9.359/build/pdf.min.js" integrity="sha256-hEmjt7z3bB53X/awJyV81gmBLpVw2mj7EsvoJelZWow=" crossorigin="anonymous">&lt;/script>




 
 
 &lt;a href="https://github.com/slds-lmu/lecture_i2dl/raw/main/cheetsheets-pdf/cheatsheet_03.pdf">
 &lt;button class="btn btn-primary" style="margin-bottom:3rem">
 Download &amp;raquo;cheatsheet_03.pdf&amp;laquo;
 &lt;/button>
 &lt;/a>


&lt;ul>
&lt;li>TensorFlow &amp;amp; PyTorch :: MLP &amp;amp; Activation Functions&lt;/li>
&lt;/ul>


&lt;script src="https://cdn.jsdelivr.net/npm/pdfjs-dist@2.9.359/build/pdf.min.js" integrity="sha256-hEmjt7z3bB53X/awJyV81gmBLpVw2mj7EsvoJelZWow=" crossorigin="anonymous">&lt;/script>




 
 
 &lt;a href="https://github.com/slds-lmu/lecture_i2dl/raw/main/cheetsheets-pdf/cheatsheet_04.pdf">
 &lt;button class="btn btn-primary" style="margin-bottom:3rem">
 Download &amp;raquo;cheatsheet_04.pdf&amp;laquo;
 &lt;/button>
 &lt;/a>


&lt;ul>
&lt;li>TensorFlow &amp;amp; PyTorch :: FCN &amp;amp; CNN&lt;/li>
&lt;/ul>


&lt;script src="https://cdn.jsdelivr.net/npm/pdfjs-dist@2.9.359/build/pdf.min.js" integrity="sha256-hEmjt7z3bB53X/awJyV81gmBLpVw2mj7EsvoJelZWow=" crossorigin="anonymous">&lt;/script>




 
 
 &lt;a href="https://github.com/slds-lmu/lecture_i2dl/raw/main/cheetsheets-pdf/cheatsheet_06.pdf">
 &lt;button class="btn btn-primary" style="margin-bottom:3rem">
 Download &amp;raquo;cheatsheet_06.pdf&amp;laquo;
 &lt;/button>
 &lt;/a>


&lt;ul>
&lt;li>TensorFlow &amp;amp; PyTorch :: Batch Normalization&lt;/li>
&lt;/ul>


&lt;script src="https://cdn.jsdelivr.net/npm/pdfjs-dist@2.9.359/build/pdf.min.js" integrity="sha256-hEmjt7z3bB53X/awJyV81gmBLpVw2mj7EsvoJelZWow=" crossorigin="anonymous">&lt;/script>




 
 
 &lt;a href="https://github.com/slds-lmu/lecture_i2dl/raw/main/cheetsheets-pdf/cheatsheet_6-2.pdf">
 &lt;button class="btn btn-primary" style="margin-bottom:3rem">
 Download &amp;raquo;cheatsheet_6-2.pdf&amp;laquo;
 &lt;/button>
 &lt;/a>


&lt;ul>
&lt;li>TensorFlow &amp;amp; PyTorch :: Modern CNN&lt;/li>
&lt;/ul>


&lt;script src="https://cdn.jsdelivr.net/npm/pdfjs-dist@2.9.359/build/pdf.min.js" integrity="sha256-hEmjt7z3bB53X/awJyV81gmBLpVw2mj7EsvoJelZWow=" crossorigin="anonymous">&lt;/script>




 
 
 &lt;a href="https://github.com/slds-lmu/lecture_i2dl/raw/main/cheetsheets-pdf/cheatsheet_6-3.pdf">
 &lt;button class="btn btn-primary" style="margin-bottom:3rem">
 Download &amp;raquo;cheatsheet_6-3.pdf&amp;laquo;
 &lt;/button>
 &lt;/a>


&lt;ul>
&lt;li>TensorFlow &amp;amp; PyTorch :: Implementation of RNNs from Scratch&lt;/li>
&lt;/ul>


&lt;script src="https://cdn.jsdelivr.net/npm/pdfjs-dist@2.9.359/build/pdf.min.js" integrity="sha256-hEmjt7z3bB53X/awJyV81gmBLpVw2mj7EsvoJelZWow=" crossorigin="anonymous">&lt;/script>




 
 
 &lt;a href="https://github.com/slds-lmu/lecture_i2dl/raw/main/cheetsheets-pdf/cheatsheet_7-1.pdf">
 &lt;button class="btn btn-primary" style="margin-bottom:3rem">
 Download &amp;raquo;cheatsheet_7-1.pdf&amp;laquo;
 &lt;/button>
 &lt;/a>


&lt;ul>
&lt;li>TensorFlow &amp;amp; PyTorch :: Modern RNNs&lt;/li>
&lt;/ul>


&lt;script src="https://cdn.jsdelivr.net/npm/pdfjs-dist@2.9.359/build/pdf.min.js" integrity="sha256-hEmjt7z3bB53X/awJyV81gmBLpVw2mj7EsvoJelZWow=" crossorigin="anonymous">&lt;/script>




 
 
 &lt;a href="https://github.com/slds-lmu/lecture_i2dl/raw/main/cheetsheets-pdf/cheatsheet_7-2.pdf">
 &lt;button class="btn btn-primary" style="margin-bottom:3rem">
 Download &amp;raquo;cheatsheet_7-2.pdf&amp;laquo;
 &lt;/button>
 &lt;/a></description></item><item><title>Errata</title><link>https://slds-lmu.github.io/i2dl/appendix/02_errata/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2dl/appendix/02_errata/</guid><description>&lt;h2 id="errata-in-the-slides-shown-in-the-videos">Errata in the slides shown in the videos&lt;/h2></description></item></channel></rss>
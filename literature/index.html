<!DOCTYPE html>
<html><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="/i2dl/css/style.css">


<title>Introduction to Machine Learning (I2ML) | Literature</title>


<link rel="apple-touch-icon" sizes="180x180" href="/i2dl/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/i2dl/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/i2dl/favicon-16x16.png">
<link rel="manifest" href="/i2dl/site.webmanifest">
<link rel="mask-icon" href="/i2dl/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">

</head><body>
<img id="logo" src="/i2dl/i2ml.svg" />

<div id="nav-border" class="container">
    <nav id="nav" class="nav justify-content-center">
        
        <a class="nav-link" href="/i2dl">
        
        Home
        </a>
        
        <a class="nav-link" href="/i2dl/chapters/">
        
        Chapters
        </a>
        
        <a class="nav-link" href="/i2dl/exercises/">
        
        Exercises
        </a>
        
        <a class="nav-link" href="/i2dl/appendix/">
        
        Appendix
        </a>
        
        <a class="nav-link" href="/i2dl/prerequisites/">
        
        Prerequisites
        </a>
        
        <a class="nav-link" href="/i2dl/literature/">
        
        Literature
        </a>
        
        <a class="nav-link" href="/i2dl/team/">
        
        Team
        </a>
        
        <a class="nav-link" href="/i2dl/news/">
        
        News
        </a>
        
    </nav>
</div><div id="content" class="container">
<h1>Literature</h1>

<p><p>The course material covers all exam-relevant topics in a quite self-contained manner.
For more in-depth study, we recommend the following literature. Note that some of the books are rather detailed and involved, and more geared towards a larger lecture in a Master&rsquo;s degree.</p>
<p>We <strong>recommend to buy and read</strong> at least one standard reference on ML, for BSc level this might be the James, for the MSc level the Hastie, Bishop, Murphy or Alplaydin, the Shalev-Shwartz for a mathematical entry point, and Deep Learning book written by Goodfelow, d2dl.ai for a python and DL entry.</p>
<h2 id="helpful-references-for-prerequisites">Helpful References for Prerequisites</h2>
<p>If you need to read up on some of the required topics (see <a href="../prerequisites">Prerequisites</a>), this list might help. We tried to keep it as short as possible.</p>
<ul>
<li>Alex J. Smola (2020): Dive into Deep Learning <a href="https://d2l.ai/index.html">URL</a> (An interactive deep learning book with code, math, and discussions Provides NumPy/MXNet, PyTorch, and TensorFlow implementations)(free HTML version)</li>
<li>Goodfellow, Bengio, Courville (2016): Deep Learning <a href="http://www.deeplearningbook.org/">URL</a> (free HTML version)</li>
<li>Awesome Deep Learning <a href="https://github.com/ChristosChristofidis/awesome-deep-learning">URL</a></li>
<li>Andrej Karpathy blog <a href="http://karpathy.github.io/">URL</a></li>
<li>Coursera Kurs &ldquo;Neural Networks for Machine Learning&rdquo; <a href="https://www.coursera.org/learn/neural-networks#syllabus">URL</a></li>
</ul>
<h3 id="good-websites-to-have-a-look">Good Websites to have a look</h3>
<ul>
<li><a href="https://distill.pub/">distill.pub</a>: in-depth explanations of important concepts, worth checking out periodically for new material</li>
</ul>
<h3 id="optimization--training-of-nns">Optimization / Training of NNs:</h3>
<ul>
<li>Why Momentum Really Works: <a href="https://distill.pub/2017/momentum/">ULR</a></li>
<li>Adam &ndash; latest trends in deep learning optimization <a href="https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6be9a291375c">URL</a></li>
<li>Overview of Gradient Descent Optimization Algorithms <a href="https://ruder.io/optimizing-gradient-descent/">URL</a></li>
<li>Yes you should understand backprop <a href="https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b">URL</a></li>
<li>A Recipe for Training Neural Networks <a href="https://karpathy.github.io/2019/04/25/recipe/">URL</a></li>
</ul>
<h3 id="regularization">Regularization:</h3>
<ul>
<li>Regularization for Deep Learning: A Taxonomy <a href="https://arxiv.org/pdf/1710.10686.pdf">URL</a></li>
</ul>
<h3 id="cnns">CNNs:</h3>
<ul>
<li>The Sobel and Laplacian Edge Detectors <a href="http://aishack.in/tutorials/sobel-laplacian-edge-detectors/">URL</a></li>
<li>Keras Blog: How convolutional neural networks see the world <a href="https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html">URL</a></li>
<li>The 9 Deep Learning Papers You Need To Know About <a href="https://adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html">URL</a></li>
<li>Python based visualization repo for CNNs <a href="https://github.com/HarisIqbal88/PlotNeuralNet">URL</a></li>
<li>Computing Receptive Fields of Convolutional Neural Networks <a href="https://distill.pub/2019/computing-receptive-fields/">URL</a></li>
<li>How Convolutional Neural Networks see the World <a href="https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html">URL</a></li>
<li>Attention in Neural Networks and How to Use It <a href="http://akosiorek.github.io/ml/2017/10/14/visual-attention.html">URL</a></li>
<li>Neural Networks - A Systematic Introduction (FU Berlin)<a href="https://page.mi.fu-berlin.de/rojas/neural/neuron.pdf">URL</a></li>
<li>Deep Learning - The Straight Dope (contains notebooks designed to teach deep learning)<a href="https://gluon.mxnet.io/">URL</a></li>
<li>Computing Receptive Fields of Convolutional Neural Networks <a href="https://distill.pub/2019/computing-receptive-fields/">URL</a></li>
<li>Stanford: Convolutional Neural Networks for Visual Recognition
<ul>
<li><a href="http://cs231n.stanford.edu/">Videos</a></li>
<li><a href="http://cs231n.github.io/">Assignements und notes</a></li>
</ul>
</li>
</ul>
<h3 id="autoencoders">Autoencoders</h3>
<ul>
<li>PCA <a href="http://www.cs.cmu.edu/~guestrin/Class/15781/slides/pca-mdps-annotated.pdf">URL</a></li>
<li>Introducing Variational Autoencoders (in Prose and Code) <a href="https://blog.fastforwardlabs.com/2016/08/12/introducing-variational-autoencoders-in-prose-and.html">URL</a></li>
</ul>
<h3 id="variational-autoencoders">Variational Autoencoders</h3>
<ul>
<li>A Tutorial on Variational Autoencoders <a href="https://arxiv.org/pdf/1606.05908.pdf">URL</a></li>
</ul>
<h3 id="reinforcement-learning">Reinforcement Learning</h3>
<ul>
<li>Statistical Reinforcement Learning (Lecture) <a href="http://nanjiang.cs.illinois.edu/cs598/">URL</a></li>
<li>Practical Reinforcement Learning (Course) <a href="https://github.com/yandexdataschool/Practical_RL">URL</a></li>
</ul>
<h3 id="lstms">LSTMs:</h3>
<ul>
<li>Understanding LSTM and its diagrams <a href="https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714">URL</a></li>
<li>The most comprehensive yet simple and fun RNN/LSTM tutorial on the Internet. <a href="https://ayearofai.com/rohan-lenny-3-recurrent-neural-networks-10300100899b">URL</a></li>
</ul>
<h3 id="hyperparameter-optimization--neural-architecture-search--etc">Hyperparameter Optimization / Neural Architecture Search / etc:</h3>
<ul>
<li>Using Machine Learning to Explore Neural Network Architecture <a href="https://ai.googleblog.com/2017/05/using-machine-learning-to-explore.html">URL</a></li>
</ul>
<h3 id="software--languages--etch">Software / Languages / etch</h3>
<ul>
<li><a href="https://towardsdatascience.com/r-vs-python-image-classification-with-keras-1fa99a8fef9b">R vs Python: Image Classification with Keras</a></li>
<li>H20 related stuff:
<ul>
<li><a href="https://github.com/ledell/sldm4-h2o">Repository that contains the H2O presentation for Trevor Hastie and Rob Tibshirani&rsquo;s Statistical Learning and Data Mining IV course in Washington, DC on October 19, 2016.</a></li>
<li><a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/booklets/DeepLearningBooklet.pdf">Deep Learning with H2O - PDF/Book</a></li>
</ul>
</li>
</ul>
<h3 id="nice-demos-and-vizualisations">Nice Demos and Vizualisations</h3>
<ul>
<li>Deep Traffic <a href="https://selfdrivingcars.mit.edu/deeptraffic/">URL</a></li>
</ul>
<h3 id="material-for-exercises">Material for Exercises</h3>
<ul>
<li>
<p>Neural networks Exercises (Part-1) <a href="https://www.r-bloggers.com/neural-networks-exercises-part-1/">URL</a></p>
</li>
<li>
<p>M. Deisenroth, A. Faisal, C. Ong. Mathematics for Machine Learning. <a href="https://mml-book.github.io/book/mml-book.pdf">URL</a></p>
</li>
<li>
<p>L. Wassermann. All of Statistics. <a href="http://egrcc.github.io/docs/math/all-of-statistics.pdf">URL</a></p>
</li>
<li>
<p>H. Wickham, G. Grolemund. R for Data Science. <a href="https://r4ds.had.co.nz/">URL</a></p>
</li>
<li>
<p>Introductory R course on datacamp.com <a href="https://learn.datacamp.com/courses/free-introduction-to-r">URL</a></p>
</li>
</ul>
<h2 id="machine-learning">Machine Learning</h2>
<ul>
<li>K. Kersting, C. Lampert, C. Rothkopf. Wie Maschinen Lernen. Springer, 2019. <a href="https://link.springer.com/book/10.1007/978-3-658-26763-6">URL</a> <em>German, informal, intuitive introduction to ML. Lower than BSc level, maybe more targeted at pupils or a non-academic audience. Read if you want a very light-weight introduction into the field, or buy as present for relatives and friends if they ask what you are doing.</em></li>
<li>G. James, D. Witten, T. Hastie, R. Tibshirani. An Introduction to Statistical Learning. MIT Press, 2010. <a href="http://www-bcf.usc.edu/~gareth/ISL/">URL</a> <em>Beginner-level introduction with applications in R. Very well suited for the BSc level.</em></li>
<li>T. Hastie, R. Tibshirani, J. Friedman. The Elements of Statistical Learning. Springer, 2009. <a href="https://web.stanford.edu/~hastie/ElemStatLearn/">URL</a> <em>Standard reference for statistics-flavored ML.</em></li>
<li>C. M. Bishop. Pattern Recognition and Machine Learning. Springer, 2006. <a href="http://research.microsoft.com/en-us/um/people/cmbishop/prml/">URL</a> <em>Standard reference for ML-flavored ML.</em></li>
<li>S. Shalev-Shwartz, S. Ben-David. Understanding machine learning: From Theory to Algorithms. Cambridge University Press, 2014. <a href="https://www.cs.huji.ac.il/w~shais/UnderstandingMachineLearning/">URL</a> <em>Great, thorough introduction to ML theory. Math-style book with definitions and proofs.</em></li>
<li>E. Alpaydin. Introduction to Machine Learning. MIT Press, 2010. <a href="http://www.cmpe.boun.edu.tr/~ethem/i2ml2e/">URL</a> <em>Standard reference with broad coverage; easy to read.</em></li>
<li>K. Murphy. Machine Learning: a Probabilistic Perspective. MIT Press, 2012. <a href="https://probml.github.io/pml-book/book0.html">URL</a> <em>Standard reference; quite extensive; statistical/probabilistic lens.</em></li>
<li>F. Provost, T. Fawcett. Data Science for Business. O’Reilly, 2013. <a href="https://book.akij.net/eBooks/2018/May/5aef50939a868/Data_Science_for_Bus.pdf">URL</a> <em>A very good, applied and easy-to-read book by 2 well-known ML scientists. Contains many practical aspects that are missing in other references. Probably a good idea to read this in any case.</em></li>
<li>N. Japkowicz. Evaluating Learning Algorithms (A Classification Perspective). Cambridge University Press, 2011. <em>Nice reading on performance measures, resampling methods and (some) statistical tests for benchmarking in ML; only for classification.</em></li>
<li>B. Bischl et al. Hyperparameter Optimization: Foundations, Algorithms, Best Practices and Open Challenges. arXiv preprint 2021. <a href="https://arxiv.org/pdf/2107.05847.pdf">URL</a> <em>Our tutorial paper on HPO.</em></li>
<li>I. Goodfellow, Y. Bengio, A. Courville. Deep Learning. MIT Press, 2016. <a href="https://www.deeplearningbook.org/">URL</a> <em>Standard, modern reference for DL.</em></li>
</ul>
<h2 id="mathematical--statistical-theory">Mathematical &amp; Statistical Theory</h2>
<ul>
<li>G. Strang. Linear Algebra and Learning from Data. Cambridge University Press, 2019. <em>Serious course on matrices and applied linear algebra.</em></li>
<li>S. Axler. Linear Algebra Done Right. Springer, 2015. <a href="https://link.springer.com/content/pdf/10.1007%2F978-3-319-11080-6.pdf">URL</a> <em>Linear Algebra from a more theoretical but still beginner-friendly perspective</em></li>
<li>A. M. Mood, F. A. Graybill, D. C. Boes. Introduction to the Theory of Statistics, McGraw-Hill 1974. <a href="https://www.fulviofrisone.com/attachments/article/446/Introduction%20to%20the%20theory%20of%20statistics%20by%20MOOD.pdf">URL</a> <em>Beginner-friendly intro to statistics; bit on the mathy side.</em></li>
<li>J. Watt, R. Borhani, A. Katsaggelos. Machine Learning Refined. Cambridge University Press, 2020. <a href="https://github.com/jermwatt/machine_learning_refined">URL</a> <em>Check chapters 2-4 plus Appendix for insightful explanations and visualizations of a variety of optimization concepts.</em></li>
<li>T. M. Cover, J. A. Thomas. Elements of Information Theory. Wiley, 2006. <a href="http://staff.ustc.edu.cn/~cgong821/Wiley.Interscience.Elements.of.Information.Theory.Jul.2006.eBook-DDU.pdf">URL</a> <em>Good intro to information theory in first hundred pages, though lacking cross-connections to ML / statistics.</em></li>
</ul>
<h2 id="python-programming">Python Programming</h2>
<ul>
<li>J. VanderPlas. Python Data Science Handbook: Essential Tools for working with Data. 2016.
Or use the online website such as:</li>
<li>Python Programming <a href="https://pythonprogramming.net/">URL</a></li>
<li>Python Tutorial <a href="https://www.pythontutorial.net/">URL</a></li>
</ul>
<h2 id="r-programming">R Programming</h2>
<ul>
<li>N. Matloff. The Art of R Programming. No Starch Press, 2011. <a href="https://diytranscriptomics.com/Reading/files/The%20Art%20of%20R%20Programming.pdf">URL</a></li>
</ul>
<p>We use the <strong>mlr3</strong> package for machine learning in R quite heavily.</p>
<ul>
<li>Central project page and learning resources: <a href="https://mlr3.mlr-org.com/">https://mlr3.mlr-org.com/</a>, in particular
<ul>
<li>the <a href="https://mlr3book.mlr-org.com/">book</a>,</li>
<li>the <a href="https://mlr3gallery.mlr-org.com/">gallery</a>, and</li>
<li>the <a href="https://cheatsheets.mlr-org.com/">cheatsheets</a>.</li>
</ul>
</li>
<li>GitHub page: <a href="https://github.com/mlr-org/mlr3">https://github.com/mlr-org/mlr3</a></li>
</ul>
</p>


<div class="chapter_overview">
<ul class="list-unstyled">

</ul>
</div>


        </div><footer class="bg-light text-center text-lg-start fixed-bottom">
<ul class="list-inline text-center">
  <li class="list-inline-item">© 2022 Course Creator</li>
  
  <li class="list-inline-item"><a class="nav-link" href="/i2dl" target="_blank">Main Course Website</a></li>
  
  <li class="list-inline-item"><a class="nav-link" href="https://github.com/slds-lmu/lecture_i2dl" target="_blank">Material Source Code</a></li>
  
  <li class="list-inline-item"><a class="nav-link" href="https://github.com/slds-lmu/i2dl" target="_blank">Website Source Code</a></li>
  
</ul>
</footer>

<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      ignoreHtmlClass: ['quizdown']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</body>
</html>
